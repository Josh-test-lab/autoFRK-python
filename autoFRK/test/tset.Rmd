---
title: "test autoFRK"
output: html_notebook
---

```{r}
library(autoFRK)
```

```{r}
# calculateLogDeterminant
R <- matrix(c(
  4.0, 1.0, 0.5,
  1.0, 3.0, 0.2,
  0.5, 0.2, 2.0
), nrow = 3, byrow = TRUE)

L <- matrix(c(
  1.0, 0.5,
  0.3, 0.7,
  0.4, 0.2
), nrow = 3, byrow = TRUE)

K <- ncol(L)  # = 2

R
L
K

# 呼叫函數
result <- calculateLogDeterminant(R, L, K)
print(result)
```

```{r}
# eigenDecompose
# 固定測試矩陣
M <- matrix(c(
  4, 2, 0,
  2, 3, 1,
  0, 1, 2
), nrow = 3, byrow = TRUE)

res_R <- eigenDecompose(M)

# 結果檢查
print(res_R)

```

```{r}
# computeLikelihood
# 固定測試資料
data <- matrix(c(
  1.0, 2.0,
  3.0, 4.0,
  5.0, 6.0
), nrow = 3, byrow = TRUE)

Fk <- matrix(c(
  1.0, 0.0,
  0.0, 1.0,
  1.0, 1.0
), nrow = 3, byrow = TRUE)

M <- matrix(c(
  2.0, 0.5,
  0.5, 1.0
), nrow = 2, byrow = TRUE)

s <- 1.5

Depsilon <- diag(c(0.1, 0.2, 0.3))

# 呼叫 computeLikelihood
result_R <- computeLikelihood(data, Fk, M, s, Depsilon)
print(result_R)

```

```{r}
# convertToPositiveDefinite
mat <- matrix(c(
  4, 2, -1,
  2, 0, 1,
  -1, 1, 3
), nrow = 3, byrow = TRUE)

# 呼叫
pd_mat_R <- convertToPositiveDefinite(mat)
print(pd_mat_R)
```

```{r}
# isDiagonal
mat1 <- matrix(c(
  1, 0, 0,
  0, 2, 0,
  0, 0, 3
), nrow = 3, byrow = TRUE)

mat2 <- matrix(c(
  1, 1, 0,
  0, 2, 0,
  0, 0, 3
), nrow = 3, byrow = TRUE)

# 呼叫
print(autoFRK::isDiagonal(mat1))  # TRUE
print(autoFRK::isDiagonal(mat2))  # FALSE
print(autoFRK::isDiagonal(5))     # TRUE

```

```{r}
# EM0miss
# -----------------------------
# 固定測試資料
# -----------------------------
# 觀測資料 (3x3)，有缺失值
data <- matrix(c(
  1, NA, 3,
  4, 5, 6,
  7, 8, 9
), nrow = 3, byrow = TRUE)

# 因子矩陣 Fk (3x3)，固定值，滿秩
Fk <- matrix(c(
  1, 0, 0,
  0, 1, 0,
  0, 0, 1
), nrow = 3, byrow = TRUE)

# 誤差矩陣 Depsilon (對角非零)
Depsilon <- diag(c(1, 1, 1))

# DfromLK 結構 (固定滿秩 wX)
DfromLK <- list(
  weights = c(1, 1, 1),
  wX = matrix(c(
    1, 0, 0,
    0, 1, 0,
    0, 0, 1
  ), nrow = 3, byrow = TRUE),
  lambda = 0.1,
  Q = diag(3),
  G = diag(3)
)

# EM 參數
maxit <- 10
avgtol <- 1e-4
vfixed <- NULL
wSave <- TRUE
verbose <- TRUE

# -----------------------------
# 呼叫 EM0miss
# -----------------------------
result <- EM0miss(
  Fk = Fk,
  data = data,
  Depsilon = Depsilon,
  maxit = maxit,
  avgtol = avgtol,
  wSave = wSave,
  DfromLK = DfromLK,
  vfixed = vfixed,
  verbose = verbose
)

# -----------------------------
# 查看結果
# -----------------------------
print(result)
```


```{python}
import torch

# -----------------------------
# 固定測試資料 (Python)
# -----------------------------

# 觀測資料 (3x3)，有缺失值
data = torch.tensor([
    [1.0, float('nan'), 3.0],
    [4.0, 5.0, 6.0],
    [7.0, 8.0, 9.0]
])

# 因子矩陣 Fk (3x3)，固定值，滿秩
Fk = torch.tensor([
    [1.0, 0.0, 0.0],
    [0.0, 1.0, 0.0],
    [0.0, 0.0, 1.0]
])

# 誤差矩陣 Depsilon (對角非零)
Depsilon = torch.diag(torch.tensor([1.0, 1.0, 1.0]))

# DfromLK 結構 (固定滿秩 wX)
DfromLK = {
    "weights": torch.tensor([1.0, 1.0, 1.0]),
    "wX": torch.tensor([
        [1.0, 0.0, 0.0],
        [0.0, 1.0, 0.0],
        [0.0, 0.0, 1.0]
    ]),
    "lambda": 0.1,
    "Q": torch.eye(3),
    "G": torch.eye(3)
}

# EM 參數
maxit = 10
avgtol = 1e-4
vfixed = None
wSave = True
verbose = True
device = 'cpu'  # 或 'cuda'

# -----------------------------
# 呼叫 EM0miss (Python)
# -----------------------------
result = EM0miss(
    Fk=Fk,
    data=data,
    Depsilon=Depsilon,
    maxit=maxit,
    avgtol=avgtol,
    wSave=wSave,
    DfromLK=DfromLK,
    vfixed=vfixed,
    verbose=verbose,
    device=device
)

# -----------------------------
# 查看結果
# -----------------------------
print(result)

```

```{r}
if ("numeric" %in% class(data)) 
    data <- as.matrix(data)
O <- !is.na(data)
TT <- NCOL(data)
ncol_Fk <- NCOL(Fk)

ziDz <- rep(NA, TT)
ziDB <- matrix(NA, TT, ncol_Fk)
db <- list()
D <- toSparseMatrix(Depsilon)
iD <- solve(D)
diagD <- isDiagonal(D)
if (!is.null(DfromLK)) {
    pick <- DfromLK$pick
    if (is.null(pick)) 
        pick <- 1:length(DfromLK$weights)
    weight <- DfromLK$weights[pick]
    DfromLK$wX <- DfromLK$wX[pick, ]
    wwX <- diag.spam(sqrt(weight)) %*% DfromLK$wX
    lQ <- DfromLK$lambda * DfromLK$Q
}
for (tt in 1:TT) {
    if (!is.null(DfromLK)) {
        iDt <- NULL
        if (sum(O[, tt]) == NROW(O)) {
            wXiG <- wwX %*% solve(DfromLK$G)
        }
        else {
            G <- t(DfromLK$wX[O[, tt], ]) %*% DfromLK$wX[O[, tt], ] + lQ
            wXiG <- wwX[O[, tt], ] %*% solve(G)
        }
        Bt <- as.matrix(Fk[O[, tt], ])
        if (NCOL(Bt) == 1) 
            Bt <- t(Bt)
        iDBt <- as.matrix(weight[O[, tt]] * Bt - wXiG %*% 
            (t(wwX[O[, tt], ]) %*% Bt))
        zt <- data[O[, tt], tt]
        ziDz[tt] <- sum(zt * as.vector(weight[O[, tt]] * zt - wXiG %*% (t(wwX[O[, tt], ]) %*% zt)))
        ziDB[tt, ] <- t(zt) %*% iDBt
        BiDBt <- t(Bt) %*% iDBt
    } else {
        if (!diagD) {
            iDt <- solve(D[O[, tt], O[, tt]])
        } else {
            iDt <- iD[O[, tt], O[, tt]]
        }
        Bt <- Fk[O[, tt], ]
        if (NCOL(Bt) == 1) 
            Bt <- t(Bt)
        iDBt <- as.matrix(iDt %*% Bt)
        zt <- data[O[, tt], tt]
        ziDz[tt] <- sum(zt * as.vector(iDt %*% zt))
        ziDB[tt, ] <- t(zt) %*% iDBt
        BiDBt <- t(Bt) %*% iDBt
    }
    db[[tt]] <- list(iDBt = iDBt, zt = zt, BiDBt = BiDBt)
}

rm(iDt, Bt, iDBt, zt, BiDBt)
gc()
dif <- Inf
cnt <- 0
Z0 <- data
Z0[is.na(Z0)] <- 0
old <- cMLEimat(Fk, Z0, s = 0, wSave = TRUE)
if (is.null(vfixed)) {
    old$s <- old$v
}else{ old$s <- vfixed}
old$M <- convertToPositiveDefinite(old$M)
Ptt1 <- old$M

while ((dif > (avgtol * (100 * ncol_Fk^2))) && (cnt < maxit)) {
    etatt <- matrix(0, ncol_Fk, TT)
    sumPtt <- 0
    s1 <- rep(0, TT)
    
    for (tt in 1:TT) {
        ginv_Ptt1 <- MASS::ginv(convertToPositiveDefinite(Ptt1))
        iP <- convertToPositiveDefinite(ginv_Ptt1 + db[[tt]]$BiDBt/old$s)
        Ptt <- solve(iP)
        Gt <- as.matrix(Ptt %*% t(db[[tt]]$iDBt)/old$s)
        eta <- c(0 + Gt %*% db[[tt]]$zt)
        s1kk <- diag(db[[tt]]$BiDBt %*% (eta %*% t(eta) + Ptt))
        
        sumPtt <- sumPtt + Ptt
        etatt[, tt] <- eta
        s1[tt] <- sum(s1kk)
    }
    if (is.null(vfixed)) {
        s <- max((sum(ziDz) - 2 * sum(ziDB * t(etatt)) + sum(s1))/sum(O), 1e-08)
        new <- list(M = (etatt %*% t(etatt) + sumPtt)/TT, s = s)
    }
    else {
        new <- list(M = (etatt %*% t(etatt) + sumPtt)/TT, s = vfixed)
    }
    new$M <- (new$M + t(new$M))/2
    dif <- sum(abs(new$M - old$M)) + abs(new$s - old$s)
    cnt <- cnt + 1
    old <- new
    Ptt1 <- old$M
}
if (verbose) 
    cat("Number of iteration: ", cnt, "\n")
n2loglik <- computeLikelihood(data, Fk, new$M, new$s, Depsilon)
if (!wSave) {
    return(list(M = new$M, s = new$s, negloglik = n2loglik))
}else if (!is.null(DfromLK)) {
    out <- list(M = new$M, s = new$s, negloglik = n2loglik, 
        w = etatt, V = new$M - etatt %*% t(etatt)/TT)
    dec <- eigen(new$M)
    L <- Fk %*% dec$vector %*% diag(sqrt(pmax(dec$value, 
        0)))
    weight <- DfromLK$weights[pick]
    wlk <- matrix(NA, NROW(lQ), TT)
    for (tt in 1:TT) {
        if (sum(O[, tt]) == NROW(O)) {
            wXiG <- wwX %*% solve(DfromLK$G)
        }
        else {
            G <- t(DfromLK$wX[O[, tt], ]) %*% DfromLK$wX[O[, 
              tt], ] + lQ
            wXiG <- wwX[O[, tt], ] %*% solve(G)
        }
        dat <- data[O[, tt], tt]
        Lt <- L[O[, tt], ]
        iDL <- weight[O[, tt]] * Lt - wXiG %*% (t(wwX[O[, tt], ]) %*% Lt)
        itmp <- solve(diag(1, NCOL(L)) + t(Lt) %*% iDL/out$s)
        iiLiD <- itmp %*% t(iDL/out$s)
        wlk[, tt] <- t(wXiG) %*% dat - t(wXiG) %*% Lt %*% 
            (iiLiD %*% dat)
    }
    attr(out, "pinfo") <- list(wlk = wlk, pick = pick)
    attr(out, "missing") <- list(miss = toSparseMatrix(1 - 
        O), maxit = maxit, avgtol = avgtol)
    return(out)
}else {
    out <- list(M = as.matrix(new$M), s = new$s, negloglik = n2loglik, 
        w = etatt, V = new$M - etatt %*% t(etatt)/TT)
    attr(out, "missing") <- list(miss = toSparseMatrix(1 - 
        O), maxit = maxit, avgtol = avgtol)
    return(out)
}
```
```{r}
data <- as.matrix(data)
Fk <- as.matrix(Fk)
num_columns <- NCOL(data)
nrow_Fk <- nrow(Fk)
ncol_Fk <- ncol(Fk)
projection <- computeProjectionMatrix(Fk, Fk, data, S)
inverse_square_root_matrix <- projection$inverse_square_root_matrix
matrix_JSJ <- projection$matrix_JSJ
sample_covariance_trace <- sum(rowSums(data^2))/num_columns
likelihood_object <- computeNegativeLikelihood(nrow_Fk = nrow_Fk, 
    ncol_Fk = ncol_Fk, s = s, p = num_columns, matrix_JSJ = matrix_JSJ, 
    sample_covariance_trace = sample_covariance_trace)
negative_log_likelihood <- likelihood_object$negative_log_likelihood
if (onlylogLike) {
    return(list(negloglik = negative_log_likelihood))
}
P <- likelihood_object$P
d_hat <- likelihood_object$d_hat
v <- likelihood_object$v
M <- inverse_square_root_matrix %*% P %*% (d_hat * t(P)) %*% 
    inverse_square_root_matrix
dimnames(M) <- NULL
if (!wSave) {
    return(list(v = v, M = M, s = s, negloglik = negative_log_likelihood))
} else {
    L <- Fk %*% t((sqrt(d_hat) * t(P)) %*% inverse_square_root_matrix)
    if (ncol_Fk > 2) {
        reduced_columns <- c(1, which(d_hat[2:ncol_Fk] > 0))
    }
    else {
        reduced_columns <- ncol_Fk
    }
    L <- L[, reduced_columns]
    invD <- rep(1, nrow_Fk)/(s + v)
    iDZ <- invD * data
    right <- L %*% (solve(diag(1, NCOL(L)) + t(L) %*% (invD * 
        L)) %*% (t(L) %*% iDZ))
    INVtZ <- iDZ - invD * right
    etatt <- as.matrix(M %*% t(Fk) %*% INVtZ)
    GM <- Fk %*% M
    V <- as.matrix(M - t(GM) %*% invCz((s + v) * diag.spam(nrow_Fk),L, GM))
    return(list(v = v, M = M, s = s, negloglik = negative_log_likelihood, 
        w = etatt, V = V))
}
```

```{r}
# invCz
# 固定測試集
R_test <- matrix(c(
  4.0, 1.0, 0.5,
  1.0, 3.0, 0.2,
  0.5, 0.2, 2.0
), nrow = 3, byrow = TRUE)

L_test <- matrix(c(
  1.0, 0.0,
  0.5, 1.0,
  0.2, 0.3
), nrow = 3, byrow = TRUE)

z_test <- c(1.0, 2.0, 3.0)

# 呼叫 invCz
result <- invCz(R_test, L_test, z_test)
print(result)

```

```{r}
# getInverseSquareRootMatrix
# 普通正定矩陣測試
A <- matrix(c(2, 0, 1,
              1, 3, 0,
              0, 1, 2), nrow=3, byrow=TRUE)

B <- matrix(c(1, 0, 1,
              0, 2, 0,
              1, 0, 3), nrow=3, byrow=TRUE)

# 呼叫函數
inv_sqrt_result <- getInverseSquareRootMatrix(A, B)
print("Inverse square root matrix result:")
print(inv_sqrt_result)
```

```{r}
# computeProjectionMatrix
# Fk1 與 Fk2
Fk1 <- matrix(c(2, 1, 0,
                0, 3, 1,
                1, 0, 2), nrow=3, byrow=TRUE)
Fk2 <- matrix(c(1, 0, 1,
                0, 2, 0,
                1, 0, 3), nrow=3, byrow=TRUE)

# data
data <- matrix(c(1, 2, 3,
                 4, 5, 6,
                 7, 8, 9), nrow=3, byrow=TRUE)

# S 為 NULL
S <- NULL

# S 非 NULL
S2 <- matrix(c(2, 0, 1,
               0, 1, 0,
               1, 0, 2), nrow=3, byrow=TRUE)

# 普通矩陣測試
result1 <- computeProjectionMatrix(Fk1 = Fk1, Fk2 = Fk2, data = data, S = NULL)
print("matrix_JSJ (S=NULL):")
print(result1$matrix_JSJ)

# 使用 S 矩陣測試
result2 <- computeProjectionMatrix(Fk1 = Fk1, Fk2 = Fk2, data = data, S = S2)
print("matrix_JSJ (S非NULL):")
print(result2$matrix_JSJ)

```

```{r}
# estimateEta
d <- c(5, 2, 7, 1)  # eigenvalues
s <- 1.5
v <- 0.5

eta <- estimateEta(d, s, v)
print(eta)  # 預期結果: c(3,0,5,0)

```

```{r}
# estimateV
d <- c(4, 2, 1)                  # eigenvalues
s <- 1.0                         # s parameter
sample_covariance_trace <- 15     # trace of sample covariance
n <- 5                            # sample size

v <- estimateV(d, s, sample_covariance_trace, n)
print(v)  # 預期結果: 正數

```
```{r}
if (max(d) < max(sample_covariance_trace/n, s)) {
    return(max(sample_covariance_trace/n - s, 0))
}
k <- length(d)
cumulative_d_values <- cumsum(d)
ks <- 1:k
if (k == n) 
    ks[n] <- n - 1
eligible_indexs <- which(d > ((sample_covariance_trace - 
    cumulative_d_values)/(n - ks)))
L <- max(eligible_indexs)
if (L >= n) 
    L <- n - 1
return(max((sample_covariance_trace - cumulative_d_values[L])/(n - L) - s, 0))
```

```{r}
# neg2llik
d <- c(4, 2, 1)
s <- 1.0
v <- 0.5
sample_covariance_trace <- 15
sample_size <- 5

result <- neg2llik(d, s, v, sample_covariance_trace, sample_size)
print(result)

```

```{r}
# computeNegativeLikelihood
# ==== 測試資料 ====
nrow_Fk <- 3
ncol_Fk <- 3
s <- 0
p <- 3

# matrix_JSJ 對應 Python tensor
matrix_JSJ <- matrix(c(
  3.3333,  7.3333, 11.3333,
  7.3333, 25.6667, 40.6667,
  11.3333, 40.6667, 64.6667
), nrow = nrow_Fk, ncol = ncol_Fk, byrow = TRUE)

# sample_covariance_trace 對應 Python tensor([93.6667])
sample_covariance_trace <- 93.6667

# 其他參數
vfixed <- NULL
ldet <- 0.0
device <- "cpu"  # 只作為標記，R 不用

# ==== 呼叫 computeNegativeLikelihood (R 版本) ====
res <- computeNegativeLikelihood(
  nrow_Fk = nrow_Fk,
  ncol_Fk = ncol_Fk,
  s = s,
  p = p,
  matrix_JSJ = matrix_JSJ,
  sample_covariance_trace = sample_covariance_trace,
  vfixed = vfixed,
  ldet = ldet
)

print(res)
```

```{r}
# cMLEimat
# 固定測試資料（R）
Fk <- matrix(c(
  1, 0, 0,
  0, 1, 0,
  0, 0, 1
), nrow = 3, byrow = FALSE)

# data: 3 x 4
data <- matrix(c(
  1, 2, 3, 4,
  2, 3, 4, 5,
  3, 4, 5, 6
), nrow = 3, byrow = TRUE)

s <- 1.0
wSave <- TRUE
S <- NULL
onlylogLike <- FALSE

# 呼叫 cMLEimat（假設函數已在 namespace 中）
result <- cMLEimat(Fk = Fk, data = data, s = s, wSave = wSave, S = S, onlylogLike = onlylogLike)

# 檢視結果
print(result$negloglik)
print(result$v)
print(dim(result$M))
# 若有 w/V：
if (!is.null(result$w)) cat("w dim:", dim(result$w), "\n")
if (!is.null(result$V)) cat("V dim:", dim(result$V), "\n")
result
```

```{r}
# computeProjectionMatrix
# ===== R 固定測試資料 =====
Fk1 <- matrix(c(
  1.0, 0.5,
  0.2, 1.2,
  0.8, 0.7
), nrow=3, byrow=TRUE)

Fk2 <- matrix(c(
  0.9, 0.3,
  0.4, 1.0,
  1.1, 0.5
), nrow=3, byrow=TRUE)

data <- matrix(c(
  1.0, 2.0, 3.0,
  0.5, 1.5, 2.5,
  2.0, 2.5, 3.5
), nrow=3, byrow=TRUE)

# 呼叫測試（S 為空）
res1 <- computeProjectionMatrix(Fk1, Fk2, data)
print(res1)

# 呼叫測試（S 不為空）
S <- diag(c(1, 2, 3))
res2 <- computeProjectionMatrix(Fk1, Fk2, data, S)
print(res2)

```

```{r}
# cMLEimat
# 固定矩陣資料
Fk <- matrix(c(
  1.0, 0.5, 0.2,
  0.3, 1.0, 0.4,
  0.1, 0.2, 1.0
), nrow = 3, ncol = 3, byrow = TRUE)

data <- matrix(c(
  2.0, 1.0, 0.5, 1.5, 2.5,
  1.2, 0.8, 0.6, 1.0, 1.8,
  0.5, 0.4, 0.3, 0.6, 0.7
), nrow = 3, ncol = 5, byrow = TRUE)

s <- 0.1

# 測試執行
res_logonly <- cMLEimat(Fk, data, s, wSave = FALSE)
res_full <- cMLEimat(Fk, data, s, wSave = TRUE)

print(res_logonly)
print(res_full)






data <- as.matrix(data)
Fk <- as.matrix(Fk)
num_columns <- NCOL(data)
nrow_Fk <- nrow(Fk)
ncol_Fk <- ncol(Fk)
projection <- computeProjectionMatrix(Fk, Fk, data, S)
inverse_square_root_matrix <- projection$inverse_square_root_matrix
matrix_JSJ <- projection$matrix_JSJ

sample_covariance_trace <- sum(rowSums(data^2))/num_columns
likelihood_object <- computeNegativeLikelihood(nrow_Fk = nrow_Fk, 
    ncol_Fk = ncol_Fk, s = s, p = num_columns, matrix_JSJ = matrix_JSJ, 
    sample_covariance_trace = sample_covariance_trace)
negative_log_likelihood <- likelihood_object$negative_log_likelihood
if (onlylogLike) {
    return(list(negloglik = negative_log_likelihood))
}
P <- likelihood_object$P
d_hat <- likelihood_object$d_hat
v <- likelihood_object$v
M <- inverse_square_root_matrix %*% P %*% (d_hat * t(P)) %*% 
    inverse_square_root_matrix
dimnames(M) <- NULL
if (!wSave) {
    return(list(v = v, M = M, s = s, negloglik = negative_log_likelihood))
} else {
    L <- Fk %*% t((sqrt(d_hat) * t(P)) %*% inverse_square_root_matrix)
    if (ncol_Fk > 2) {
        reduced_columns <- c(1, which(d_hat[2:ncol_Fk] > 0))
    } else {
        reduced_columns <- ncol_Fk
    }
    L <- L[, reduced_columns]
    invD <- rep(1, nrow_Fk)/(s + v)
    iDZ <- invD * data
    right <- L %*% (solve(diag(1, NCOL(L)) + t(L) %*% (invD * 
        L)) %*% (t(L) %*% iDZ))
    INVtZ <- iDZ - invD * right
    etatt <- as.matrix(M %*% t(Fk) %*% INVtZ)
    GM <- Fk %*% M
    V <- as.matrix(M - t(GM) %*% invCz((s + v) * diag.spam(nrow_Fk),L, GM))
    return(list(v = v, M = M, s = s, negloglik = negative_log_likelihood, 
        w = etatt, V = V))
}
```

```{r}
# cMLE
# 固定測試資料
Fk <- matrix(c(
  0.1, 0.2, 0.3,
  0.4, 0.5, 0.6,
  0.7, 0.8, 0.9,
  1.0, 1.1, 1.2,
  1.3, 1.4, 1.5
), nrow = 5, byrow = TRUE)

Z0 <- matrix(c(
  1, 0, 2,
  2, 3, 0,
  0, 1, 4,
  3, 2, 1,
  4, 3, 0
), nrow = 5, byrow = TRUE)

proj <- computeProjectionMatrix(Fk, Fk, Z0)

cMLE(
  Fk = Fk,
  num_columns = ncol(Z0),
  sample_covariance_trace = sum(Z0^2)/ncol(Z0),
  inverse_square_root_matrix = proj$inverse_square_root_matrix,
  matrix_JSJ = proj$matrix_JSJ,
  s = 0,
  wSave = TRUE
)

```

```{python}
import torch

device = "cpu"

Fk = torch.tensor([
    [0.1, 0.2, 0.3],
    [0.4, 0.5, 0.6],
    [0.7, 0.8, 0.9],
    [1.0, 1.1, 1.2],
    [1.3, 1.4, 1.5]
], dtype=torch.float32, device=device)

Z0 = torch.tensor([
    [1, 0, 2],
    [2, 3, 0],
    [0, 1, 4],
    [3, 2, 1],
    [4, 3, 0]
], dtype=torch.float32, device=device)

proj = computeProjectionMatrix(Fk, Fk, Z0, device=device)

cMLE(
    Fk = Fk,
    num_columns = Z0.shape[1],
    sample_covariance_trace = torch.sum(Z0**2)/Z0.shape[1],
    inverse_square_root_matrix = proj['inverse_square_root_matrix'],
    matrix_JSJ = proj['matrix_JSJ'],
    s = 0,
    wSave = True,
    device=device
)

```

```{r}
function (Fk, num_columns, sample_covariance_trace, inverse_square_root_matrix, 
    matrix_JSJ, s = 0, ldet = 0, wSave = FALSE, onlylogLike = !wSave, 
    vfixed = NULL) 
{
    nrow_Fk <- nrow(Fk)
    likelihood_object <- computeNegativeLikelihood(nrow_Fk = nrow_Fk, 
        ncol_Fk = ncol(Fk), s = s, p = num_columns, matrix_JSJ = matrix_JSJ, 
        sample_covariance_trace = sample_covariance_trace, vfixed = vfixed, 
        ldet = ldet)
    negative_log_likelihood <- likelihood_object$negative_log_likelihood
    if (onlylogLike) {
        return(list(negloglik = negative_log_likelihood))
    }
    P <- likelihood_object$P
    d_hat <- likelihood_object$d_hat
    M <- inverse_square_root_matrix %*% P %*% (d_hat * t(P)) %*% 
        inverse_square_root_matrix
    dimnames(M) <- NULL
    if (!wSave) {
        L <- NULL
    }
    else {
        if (d_hat[1] != 0) {
            L <- Fk %*% ((sqrt(d_hat) * t(P)) %*% inverse_square_root_matrix)
            L <- as.matrix(L[, d_hat > 0])
        }
        else {
            L <- matrix(0, nrow_Fk, 1)
        }
    }
    return(list(v = likelihood_object$v, M = M, s = s, negloglik = negative_log_likelihood, 
        L = L))
}
```

```{r}
# 

```

```{r}
# 

```

```{r}
# 

```






















