{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eacff167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import torch\n",
    "import gc\n",
    "from pprint import pprint\n",
    "\n",
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype) \n",
    "\n",
    "def clear():\n",
    "    \"\"\"清空終端畫面\"\"\"\n",
    "    if platform.system() == \"Windows\":\n",
    "        os.system(\"cls\")\n",
    "    else:\n",
    "        os.system(\"clear\")\n",
    "cls = clear\n",
    "\n",
    "def clear_all():\n",
    "    \"\"\"\n",
    "    清除全域變數中常用的資料型態變數 (int, float, str, list, dict, torch.Tensor)\n",
    "    \"\"\"\n",
    "    for name, val in list(globals().items()):\n",
    "        if isinstance(val, (int, float, str, list, dict, torch.Tensor)):\n",
    "            del globals()[name]\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"\n",
    "    強制垃圾回收並清空 PyTorch CUDA 記憶體\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "cm = cleanup_memory\n",
    "\n",
    "def convert_all_tensors_dtype(dtype=dtype):\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            if obj.dtype != dtype:\n",
    "                obj.data = obj.data.to(dtype=dtype)\n",
    "\n",
    "def p(obj):\n",
    "    \"\"\"\n",
    "    遞迴漂亮印出 dict / list / tensor。\n",
    "    tensor 會換行縮排對齊，方便查看。\n",
    "    \"\"\"\n",
    "    def pretty_tensor(tensor: torch.Tensor, indent: int = 6) -> str:\n",
    "        lines = str(tensor).split(\"\\n\")\n",
    "        if len(lines) == 1:\n",
    "            return f\"{lines[0]}\"\n",
    "        ind = \" \" * indent\n",
    "        return \"\\n\" + \"\\n\".join(ind + line for line in lines) + \"\\n\" + \" \" * (indent - 2)\n",
    "\n",
    "    def _p(obj, indent=0):\n",
    "        space = \" \" * indent\n",
    "        if isinstance(obj, dict):\n",
    "            print(space + \"{\")\n",
    "            for k, v in obj.items():\n",
    "                print(f\"{space}  {repr(k)}: \", end=\"\")\n",
    "                _p(v, indent + 4)\n",
    "            print(space + \"}\")\n",
    "        elif isinstance(obj, list) or isinstance(obj, tuple):\n",
    "            print(space + \"[\")\n",
    "            for v in obj:\n",
    "                _p(v, indent + 4)\n",
    "            print(space + \"]\")\n",
    "        elif isinstance(obj, torch.Tensor):\n",
    "            print(pretty_tensor(obj, indent + 4))\n",
    "        else:\n",
    "            print(repr(obj))\n",
    "\n",
    "    _p(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7f067d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22724\\2859990534.py:36: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  if isinstance(obj, torch.Tensor):\n"
     ]
    }
   ],
   "source": [
    "convert_all_tensors_dtype(dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb24cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import colorlog\n",
    "\n",
    "# logger config\n",
    "def setup_logger() -> logging.Logger:\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Check if logger has already been configured\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Create a formatter\n",
    "    formatter = colorlog.ColoredFormatter(\n",
    "        fmt='%(log_color)s%(asctime)s - %(name)s - %(levelname)s: %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        log_colors={\n",
    "            'DEBUG': 'cyan',\n",
    "            'INFO': 'green',\n",
    "            'WARNING': 'yellow',\n",
    "            'ERROR': 'red',\n",
    "            'CRITICAL': 'bold_red',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create a console handler\n",
    "    console_handler = colorlog.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the handler to the logger\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd77935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Setup file of autoFRK-Python Project\n",
    "Author: Hsu, Yao-Chih\n",
    "Version: 1141007\n",
    "Reference:\n",
    "\"\"\"\n",
    "\n",
    "# development only\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"./src\"))\n",
    "\n",
    "# import modules\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import gc\n",
    "from typing import Optional, Union\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# logger config\n",
    "LOGGER = setup_logger()\n",
    "\n",
    "# fast mode KNN for missing data imputation, using in autoFRK\n",
    "# Its have OpenMP issue, set environment variable OMP_NUM_THREADS=1 to avoid it, or use sklearn version below\n",
    "# check = ok\n",
    "def fast_mode_knn(\n",
    "    data: torch.Tensor,\n",
    "    loc: torch.Tensor, \n",
    "    n_neighbor: int = 3\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    The fast mode for autoFRK by using KNN for missing data imputation.\n",
    "\n",
    "    Parameters:\n",
    "        data: (N, T) or (samples, time_points) tensor.\n",
    "        loc: (N, spatial_dim) tensor, e.g., 2D space N x 2.\n",
    "        n_neighbor: Number of neighbors to use for KNN.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The data tensor with missing values imputed.\n",
    "    \"\"\"\n",
    "    dtype=data.dtype\n",
    "    device=data.device\n",
    "\n",
    "    data = data.detach().cpu().numpy()\n",
    "    loc = loc.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # use faiss on GPU if available\n",
    "    if device.type != 'cpu':\n",
    "        res = faiss.StandardGpuResources()\n",
    "\n",
    "    for tt in range(data.shape[1]):\n",
    "        col = data[:, tt]\n",
    "        where = np.isnan(col)\n",
    "        if not np.any(where):\n",
    "            continue\n",
    "\n",
    "        known_idx = np.where(~where)[0]\n",
    "        unknown_idx = np.where(where)[0]\n",
    "\n",
    "        # if low known values\n",
    "        if len(known_idx) < n_neighbor:\n",
    "            err_msg = f'Column {tt} has too few known values to impute ({len(known_idx)} < {n_neighbor}).'\n",
    "            LOGGER.warning(err_msg)\n",
    "            raise ValueError(err_msg)\n",
    "\n",
    "        # use faiss for KNN\n",
    "        index = faiss.IndexFlatL2(loc.shape[1])\n",
    "        if device.type != 'cpu':\n",
    "            index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "        # get the values of neighbors\n",
    "        index.add(loc[known_idx])\n",
    "        _, knn_idx = index.search(loc[unknown_idx], n_neighbor)\n",
    "\n",
    "        # impute missing values with the mean of neighbors\n",
    "        neighbor_vals = col[known_idx[knn_idx]]\n",
    "        col[where] = np.nanmean(neighbor_vals, axis=1)\n",
    "        data[:, tt] = col\n",
    "\n",
    "    return torch.tensor(data, dtype=dtype, device=device)\n",
    "\n",
    "# fast mode KNN for missing data imputation, using in autoFRK, sklearn version\n",
    "# check = ok\n",
    "def fast_mode_knn_sklearn(\n",
    "    data: torch.Tensor,\n",
    "    loc: torch.Tensor,\n",
    "    n_neighbor: int = 3\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    dtype = data.dtype\n",
    "    device = data.device\n",
    "\n",
    "    data = data.detach().cpu().numpy()\n",
    "    loc = loc.detach().cpu().numpy()\n",
    "\n",
    "    for tt in range(data.shape[1]):\n",
    "        col = data[:, tt]\n",
    "        where = np.isnan(col)\n",
    "        if not np.any(where):\n",
    "            continue\n",
    "\n",
    "        known_idx = np.where(~where)[0]\n",
    "        unknown_idx = np.where(where)[0]\n",
    "\n",
    "        if 0 < len(known_idx) < n_neighbor:\n",
    "            err_msg = f'Column {tt} has too few known values to impute ({len(known_idx)} < {n_neighbor}).'\n",
    "            LOGGER.warning(err_msg)\n",
    "            raise ValueError(err_msg)\n",
    "\n",
    "        knn = NearestNeighbors(n_neighbors=n_neighbor, algorithm='auto').fit(loc[known_idx])\n",
    "        distances, knn_idx = knn.kneighbors(loc[unknown_idx])\n",
    "\n",
    "        neighbor_vals = col[known_idx[knn_idx]]\n",
    "        col[where] = np.nanmean(neighbor_vals, axis=1)\n",
    "        data[:, tt] = col\n",
    "\n",
    "    return torch.tensor(data, dtype=dtype, device=device)\n",
    "\n",
    "# select basis function for autoFRK, using in autoFRK\n",
    "# check = none\n",
    "def selectBasis(\n",
    "    data: torch.Tensor,\n",
    "    loc: torch.Tensor,\n",
    "    D: torch.Tensor = None,\n",
    "    maxit: int = 50,\n",
    "    avgtol: float = 1e-6,\n",
    "    max_rank: int = None,\n",
    "    sequence_rank: torch.Tensor = None,\n",
    "    method: str = \"fast\",\n",
    "    num_neighbors: int = 3,\n",
    "    max_knot: int = 5000,\n",
    "    DfromLK: dict = None,\n",
    "    Fk: torch.Tensor = None,\n",
    "    device: Optional[Union[torch.device, str]] = 'cpu'\n",
    ") -> torch.Tensor:\n",
    "    # 去除全為 NaN 的欄位\n",
    "    not_all_nan = ~torch.isnan(data).all(dim=0)\n",
    "    data = data[:, not_all_nan]\n",
    "\n",
    "    # 檢查資料中是否有缺失值\n",
    "    is_data_with_missing_values = torch.isnan(data).any()\n",
    "\n",
    "    # 找出整行都是 NaN 的列（完全缺失）\n",
    "    na_rows = torch.isnan(data).all(dim=1)\n",
    "    pick = torch.arange(data.shape[0])\n",
    "    if na_rows.any():\n",
    "        data = data[~na_rows]\n",
    "        loc = loc[~na_rows]  # 同步刪除 loc 中相同的行 need fix\n",
    "        D = D[~na_rows][:, ~na_rows]\n",
    "        pick = pick[~na_rows]\n",
    "        is_data_with_missing_values = torch.isnan(data).any()\n",
    "\n",
    "    # 如果 D 未提供，則初始化為單位對角矩陣\n",
    "    if D is None:\n",
    "        D = torch.eye(data.shape[0], device=data.device)\n",
    "\n",
    "    # 取得位置維度\n",
    "    d = loc.shape[1]\n",
    "\n",
    "    # 計算 klim 與選 knot\n",
    "    N = len(pick)\n",
    "    klim = int(min(N, np.round(10 * np.sqrt(N))))\n",
    "    if N < max_knot:\n",
    "        knot = loc[pick, :]\n",
    "    else:\n",
    "        knot = subKnot(x=loc[pick, :],\n",
    "                       nknot=min(max_knot, klim),\n",
    "                       device=device\n",
    "                       ).to(device=device)\n",
    "\n",
    "    # 處理 K 值\n",
    "    if max_rank is not None:\n",
    "        max_rank = round(max_rank)\n",
    "    else:\n",
    "        max_rank = torch.round(torch.max(sequence_rank)).to(torch.int) if sequence_rank is not None else klim\n",
    "\n",
    "    if sequence_rank is not None:\n",
    "        K = torch.unique(torch.round(sequence_rank).to(torch.int))\n",
    "        if K.max() > max_rank:\n",
    "            err_msg = f'maximum of sequence_rank is larger than max_rank!'\n",
    "            LOGGER.error(err_msg)\n",
    "            raise ValueError(err_msg)\n",
    "        elif torch.all(K <= d):\n",
    "            err_msg = f'Not valid sequence_rank!'\n",
    "            LOGGER.error(err_msg)\n",
    "            raise ValueError(err_msg)\n",
    "        elif torch.any(K < (d + 1)):\n",
    "            warn_msg = f'The minimum of sequence_rank can not less than {d + 1}. Too small values will be ignored.'\n",
    "            LOGGER.warning(warn_msg)\n",
    "        K = K[K > d]\n",
    "    else:\n",
    "        step = max_rank ** (1/3) * d\n",
    "        K = torch.arange(d + 1, max_rank, step).round().to(torch.int).unique()\n",
    "        if len(K) > 30:\n",
    "            K = torch.linspace(d + 1, max_rank, 30).round().to(torch.int).unique()\n",
    "\n",
    "    # Fk 為 None 時初始化 basis function 值\n",
    "    if Fk is None:\n",
    "        mrts = MRTS(locs=loc, k=max(K), device=device)  # 待修 (knot, max(K), loc, max_knot) need fix\n",
    "        Fk = mrts.forward()\n",
    "\n",
    "    AIC_list = [float('inf')] * len(K)\n",
    "    num_data_columns = data.shape[1]\n",
    "\n",
    "    if method == \"EM\" and DfromLK is None:\n",
    "        for k in range(len(K)):\n",
    "            AIC_list[k] = indeMLE(data,\n",
    "                                  Fk[pick, :K[k]],\n",
    "                                  D,\n",
    "                                  maxit,\n",
    "                                  avgtol,\n",
    "                                  wSave=False,\n",
    "                                  verbose=False\n",
    "                                  )[\"negloglik\"]\n",
    "    else:\n",
    "        if is_data_with_missing_values:\n",
    "            data = fast_mode_knn_sklearn(data=data, loc=loc, n_neighbor=num_neighbors) \n",
    "        if DfromLK is None:\n",
    "            iD = torch.linalg.solve(D, torch.eye(D.shape[0], device=D.device))\n",
    "            iDFk = iD @ Fk[pick, :]\n",
    "            iDZ = iD @ data\n",
    "        else:\n",
    "            wX = DfromLK[\"wX\"][pick, :]\n",
    "            G = DfromLK[\"wX\"].T @ DfromLK[\"wX\"] + DfromLK[\"lambda\"] * DfromLK[\"Q\"]\n",
    "            weight = DfromLK[\"weights\"][pick]\n",
    "            wwX = torch.diag(torch.sqrt(weight)) @ wX\n",
    "            wXiG = torch.linalg.solve(G, wwX.T).T\n",
    "            iDFk = weight * Fk[pick, :] - wXiG @ (wwX.T @ Fk[pick, :])\n",
    "            iDZ = weight * data - wXiG @ (wwX.T @ data)\n",
    "\n",
    "        sample_covariance_trace = torch.sum(iDZ * data) / num_data_columns\n",
    "\n",
    "        for k in range(len(K)):\n",
    "            Fk_k = Fk[pick, :K[k]]\n",
    "            iDFk_k = iDFk[:, :K[k]]\n",
    "            inverse_square_root_matrix = get_inverse_square_root_matrix(Fk_k, iDFk_k)\n",
    "            ihFiD = inverse_square_root_matrix @ iDFk_k.T\n",
    "            tmp = torch.matmul(ihFiD, data)\n",
    "            matrix_JSJ = torch.matmul(tmp, tmp.T) / num_data_columns\n",
    "            matrix_JSJ = (matrix_JSJ + matrix_JSJ.T) / 2\n",
    "            AIC_list[k] = cMLE(Fk=Fk_k,\n",
    "                               num_columns=num_data_columns,\n",
    "                               sample_covariance_trace=sample_covariance_trace,\n",
    "                               inverse_square_root_matrix=inverse_square_root_matrix,\n",
    "                               matrix_JSJ=matrix_JSJ\n",
    "                               )[\"negloglik\"]\n",
    "\n",
    "    # 計算 AIC 並選出最佳 K 值\n",
    "    df = torch.where(\n",
    "        K <= num_data_columns,\n",
    "        (K * (K + 1) / 2 + 1),\n",
    "        (K * num_data_columns + 1 - num_data_columns * (num_data_columns - 1) / 2)\n",
    "    )\n",
    "\n",
    "    AIC_list = AIC_list + 2 * df\n",
    "    Kopt = K[torch.argmin(AIC_list)].item()\n",
    "    out = Fk[:, :Kopt]\n",
    "    return out\n",
    "\n",
    "# check = ok\n",
    "def get_inverse_square_root_matrix(left_matrix, right_matrix):\n",
    "    mat = left_matrix.T @ right_matrix  # A^T * B\n",
    "    mat = (mat + mat.T) / 2\n",
    "    eigvals, eigvecs = torch.linalg.eigh(mat)\n",
    "    inv_sqrt_eigvals = torch.diag(torch.clamp(eigvals, min=1e-10).rsqrt())\n",
    "    return eigvecs @ inv_sqrt_eigvals @ eigvecs.T\n",
    "\n",
    "# subset knot selection for autoFRK, using in selectBasis\n",
    "# check = ok\n",
    "def subKnot(\n",
    "    x: torch.Tensor, \n",
    "    nknot: int, \n",
    "    xrng: torch.Tensor = None, \n",
    "    nsamp: int = 1, \n",
    "    device: Optional[Union[torch.device, str]]='cpu'\n",
    ") -> torch.Tensor:\n",
    "    x = x.to(device)\n",
    "    x = torch.sort(x, dim=0).values\n",
    "    xdim = x.shape  # (N, D)\n",
    "\n",
    "    if xrng is None:\n",
    "        xrng = torch.stack([x.min(dim=0).values, x.max(dim=0).values], dim=0)\n",
    "\n",
    "    rng = torch.sqrt(xrng[1] - xrng[0])\n",
    "    if (rng == 0).any():\n",
    "        rng[rng == 0] = rng[rng > 0].min() / 5\n",
    "    rng = rng * 10 / rng.min()\n",
    "    rng_max_index = torch.argmax(rng).item()\n",
    "\n",
    "    log_rng = torch.log(rng)\n",
    "    nmbin = torch.round(torch.exp(log_rng * torch.log(torch.tensor(nknot, dtype=torch.float32)) / log_rng.sum())).int()\n",
    "    nmbin = torch.clamp(nmbin, min=2)\n",
    "\n",
    "    while torch.prod(nmbin).item() < nknot:\n",
    "        nmbin[rng_max_index] += 1\n",
    "\n",
    "    gvec = torch.ones(xdim[0], dtype=torch.int64, device=device)\n",
    "    cnt = 0\n",
    "    while len(torch.unique(gvec)) < nknot:\n",
    "        nmbin += cnt\n",
    "        kconst = 1\n",
    "        gvec = torch.ones(xdim[0], dtype=torch.int64, device=device)\n",
    "        for kk in range(xdim[1]):\n",
    "            delta = xrng[1, kk] - xrng[0, kk]\n",
    "            if delta == 0:\n",
    "                grp = torch.zeros(xdim[0], dtype=torch.int64, device=device)\n",
    "            else:\n",
    "                grp = ((nmbin[kk] - 1) * (x[:, kk] - xrng[0, kk]) / delta).round().int()\n",
    "                grp = torch.clamp(grp, max=nmbin[kk] - 1)\n",
    "\n",
    "            if len(torch.unique(grp)) < nmbin[kk]:\n",
    "                brk = torch.tensor(np.quantile(x[:, kk].cpu().numpy(), np.linspace(0, 1, nmbin[kk] + 1)), device=device)\n",
    "                brk[0] -= 1e-8\n",
    "                grp = torch.bucketize(x[:, kk], brk) - 1\n",
    "            gvec += kconst * grp\n",
    "            kconst *= nmbin[kk]\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    gvec_np = gvec.cpu().numpy()\n",
    "    index = []\n",
    "    for g in np.unique(gvec_np):\n",
    "        idx = np.where(gvec_np == g)[0]\n",
    "        if len(idx) == 1:\n",
    "            index.append(idx[0])\n",
    "        else:\n",
    "            np.random.seed(int(np.mean(idx)))\n",
    "            index.extend(np.random.choice(idx, size=min(nsamp, len(idx)), replace=False))\n",
    "\n",
    "    index = torch.tensor(index, device=device)\n",
    "    return x[index]\n",
    "\n",
    "# compute negative log likelihood for autoFRK, using in selectBasis\n",
    "# check = none\n",
    "def cMLE(\n",
    "    Fk: torch.Tensor,\n",
    "    num_columns: int,\n",
    "    sample_covariance_trace: float,\n",
    "    inverse_square_root_matrix: torch.Tensor,\n",
    "    matrix_JSJ: torch.Tensor,\n",
    "    s: float = 0,\n",
    "    ldet: float = 0,\n",
    "    wSave: bool = False,\n",
    "    onlylogLike: bool = None,\n",
    "    vfixed: float = None,\n",
    "    device: Optional[Union[torch.device, str]] = 'cpu'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Internal function: maximum likelihood estimate with the likelihood\n",
    "\n",
    "    Parameters:\n",
    "        Fk: (N, K) torch.Tensor, basis functions.\n",
    "        num_columns: (int) Number of columns in the data.\n",
    "        sample_covariance_trace: (float) Trace of the sample covariance matrix.\n",
    "        inverse_square_root_matrix: (N, K) torch.Tensor, inverse square root matrix.\n",
    "        matrix_JSJ: (K, K) torch.Tensor, covariance-like matrix.\n",
    "        s: (float) Effective sample size, default is 0.\n",
    "        ldet: (float) Log determinant of the transformation matrix, default is 0.\n",
    "        wSave: (bool) Whether to save the L matrix, default is False.\n",
    "        onlylogLike: (bool) If True, only return the negative log likelihood.\n",
    "        vfixed: (float, optional) Fixed noise variance, if provided.\n",
    "        device: (str) 'cpu' or 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'v': (float) Estimated noise variance,\n",
    "            'M': (torch.Tensor) Matrix M,\n",
    "            's': (int) Effective sample size,\n",
    "            'negloglik': (float) Negative log likelihood,\n",
    "            'L': (torch.Tensor, optional) L matrix if wSave is True.\n",
    "        }\n",
    "    \"\"\"\n",
    "    nrow_Fk = Fk.shape[0]\n",
    "\n",
    "    likelihood_object = computeNegativeLikelihood(\n",
    "        nrow_Fk=nrow_Fk,\n",
    "        ncol_Fk=Fk.shape[1],\n",
    "        s=s,\n",
    "        p=num_columns,\n",
    "        matrix_JSJ=matrix_JSJ,\n",
    "        sample_covariance_trace=sample_covariance_trace,\n",
    "        vfixed=vfixed,\n",
    "        ldet=ldet,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    negative_log_likelihood = likelihood_object['negative_log_likelihood']\n",
    "\n",
    "    if onlylogLike:\n",
    "        return {'negloglik': negative_log_likelihood}\n",
    "\n",
    "    P = likelihood_object['P']\n",
    "    d_hat = likelihood_object['d_hat']\n",
    "    v = likelihood_object['v']\n",
    "    M = inverse_square_root_matrix @ P @ (d_hat * P.T) @ inverse_square_root_matrix\n",
    "\n",
    "    if not wSave:\n",
    "        L = None\n",
    "    elif d_hat[0] != 0:\n",
    "        L = Fk @ ((torch.sqrt(d_hat) * P.T) @ inverse_square_root_matrix)\n",
    "        L = L[:, d_hat > 0]\n",
    "    else:\n",
    "        L = torch.zeros((nrow_Fk, 1), dtype=Fk.dtype, device=Fk.device)\n",
    "\n",
    "    return {'v': v,\n",
    "            'M': M,\n",
    "            's': s,\n",
    "            'negloglik': negative_log_likelihood,\n",
    "            'L': L\n",
    "            }\n",
    "\n",
    "# compute negative log likelihood for autoFRK, using in cMLE\n",
    "# check = ok\n",
    "def computeNegativeLikelihood(\n",
    "    nrow_Fk: int,\n",
    "    ncol_Fk: int,\n",
    "    s: int,\n",
    "    p: int,\n",
    "    matrix_JSJ: torch.Tensor,\n",
    "    sample_covariance_trace: float,\n",
    "    vfixed: float = None,\n",
    "    ldet: float = 0.0,\n",
    "    device: Optional[Union[torch.device, str]]='cpu'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute negative log-likelihood.\n",
    "    \n",
    "    Parameters:\n",
    "        nrow_Fk: (int) Number of rows in basis matrix Fk.\n",
    "        ncol_Fk: (int) Number of basis functions (columns of Fk).\n",
    "        s: (int) Effective sample size.\n",
    "        p: (int) Number of variables (e.g. number of spatial points).\n",
    "        matrix_JSJ: (torch.Tensor) Covariance-like matrix (should be symmetric).\n",
    "        sample_covariance_trace: (float) Trace of sample covariance matrix.\n",
    "        vfixed: (float, optional) Fixed noise variance (if provided).\n",
    "        ldet: (float, optional) Log determinant of transformation matrix.\n",
    "        device: (str) 'cpu' or 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'negative_log_likelihood': float,\n",
    "            'P': torch.Tensor (eigenvectors),\n",
    "            'v': float,\n",
    "            'd_hat': torch.Tensor\n",
    "        }\n",
    "    \"\"\"\n",
    "    matrix_JSJ = matrix_JSJ.to(device)\n",
    "\n",
    "    if not torch.allclose(matrix_JSJ, matrix_JSJ.T, atol=1e-10):\n",
    "        err_msg = f'Please input a symmetric matrix'\n",
    "        LOGGER.error(err_msg)\n",
    "        raise ValueError(err_msg)\n",
    "\n",
    "    if matrix_JSJ.size(1) < ncol_Fk:\n",
    "        err_msg = f'Please input the rank of a matrix larger than ncol_Fk = {ncol_Fk}'\n",
    "        LOGGER.error(err_msg)\n",
    "        raise ValueError(err_msg)\n",
    "\n",
    "    eigenvalues_JSJ, eigenvectors_JSJ = torch.linalg.eigh(matrix_JSJ)\n",
    "    idx = torch.argsort(eigenvalues_JSJ, descending=True)\n",
    "    eigenvalues_JSJ = eigenvalues_JSJ[idx][:ncol_Fk]\n",
    "    eigenvectors_JSJ = eigenvectors_JSJ[:, idx][:, :ncol_Fk]\n",
    "\n",
    "    if vfixed is None:\n",
    "        v = estimateV(d=eigenvalues_JSJ, \n",
    "                      s=s, \n",
    "                      sample_covariance_trace=sample_covariance_trace, \n",
    "                      n=nrow_Fk\n",
    "                      )\n",
    "    else:\n",
    "        v = vfixed\n",
    "\n",
    "    d = torch.clamp(eigenvalues_JSJ, min=0)\n",
    "    d_hat = estimateEta(d, s, v)\n",
    "\n",
    "    negative_log_likelihood = neg2llik(d=d, \n",
    "                                       s=s, \n",
    "                                       v=v, \n",
    "                                       sample_covariance_trace=sample_covariance_trace, \n",
    "                                       sample_size=nrow_Fk\n",
    "                                       ) * p + ldet * p\n",
    "\n",
    "    return {\"negative_log_likelihood\": negative_log_likelihood,\n",
    "            \"P\": eigenvectors_JSJ,\n",
    "            \"v\": v,\n",
    "            \"d_hat\": d_hat\n",
    "            }\n",
    "\n",
    "# estimate the eta parameter for negative likelihood, using in computeNegativeLikelihood\n",
    "# check = ok\n",
    "def estimateV(\n",
    "    d: torch.Tensor, \n",
    "    s: float, \n",
    "    sample_covariance_trace: float, \n",
    "    n: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Estimate the v parameter.\n",
    "\n",
    "    Parameters:\n",
    "        d: (torch.Tensor) 1D tensor of nonnegative eigenvalues (length k)\n",
    "        s: (float) A positive numeric constant\n",
    "        sample_covariance_trace: (float) Trace of sample covariance\n",
    "        n: (int) Sample size\n",
    "\n",
    "    Returns:\n",
    "        v: (float) Estimated noise variance\n",
    "    \"\"\"\n",
    "    if torch.max(d) < max(sample_covariance_trace / n, s):\n",
    "        return max(sample_covariance_trace / n - s, 0.0)\n",
    "\n",
    "    k = d.shape[0]\n",
    "    cumulative_d_values = torch.cumsum(d, dim=0)\n",
    "    ks = torch.arange(1, k + 1, device=d.device)\n",
    "    if k == n:\n",
    "        ks[-1] = n - 1\n",
    "\n",
    "    eligible_indices = torch.nonzero(d > (sample_covariance_trace - cumulative_d_values) / (n - ks)).flatten()\n",
    "    \n",
    "    if len(eligible_indices) == 0:\n",
    "        error_msg = \"No eligible indices found: check input d, sample_covariance_trace, and n.\"\n",
    "        LOGGER.error(error_msg)\n",
    "        raise ValueError(error_msg)\n",
    "    L = int(torch.max(eligible_indices))\n",
    "\n",
    "    if (L + 1) >= n:\n",
    "        L = n - 1\n",
    "        v_hat = max((sample_covariance_trace - cumulative_d_values[L - 1]) / (n - L) - s, 0.0)\n",
    "    else:\n",
    "        v_hat = max((sample_covariance_trace - cumulative_d_values[L]) / (n - L - 1) - s, 0.0)\n",
    "    return v_hat\n",
    "\n",
    "# estimate the eta parameter for negative likelihood, using in computeNegativeLikelihood\n",
    "# check = ok\n",
    "def estimateEta(\n",
    "    d: torch.Tensor, \n",
    "    s: float, \n",
    "    v: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Estimate the eta parameter.\n",
    "\n",
    "    Parameters:\n",
    "        d: (torch.Tensor) 1D tensor of nonnegative values (eigenvalues)\n",
    "        s: (float) A positive numeric\n",
    "        v: (float) A positive numeric\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of estimated eta values\n",
    "    \"\"\"\n",
    "    return torch.clamp(d - s - v, min=0.0)\n",
    "\n",
    "# compute the negative log likelihood, using in computeNegativeLikelihood\n",
    "# check = ok\n",
    "def neg2llik(\n",
    "    d: torch.Tensor,\n",
    "    s: float,\n",
    "    v: float,\n",
    "    sample_covariance_trace: float,\n",
    "    sample_size: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Estimate the negative log-likelihood (up to constant)\n",
    "\n",
    "    Parameters:\n",
    "        d: Tensor of nonnegative values (eigenvalues)\n",
    "        s: A positive scalar\n",
    "        v: A positive scalar\n",
    "        sample_covariance_trace: Scalar trace value\n",
    "        sample_size: Number of samples (int)\n",
    "\n",
    "    Returns:\n",
    "        Scalar negative log-likelihood value\n",
    "    \"\"\"\n",
    "    k = d.shape[0]\n",
    "    eta = estimateEta(d, s, v)\n",
    "\n",
    "    if torch.max(eta / (s + v)) > 1e20:\n",
    "        return float(\"inf\")\n",
    "    sPlusv = torch.as_tensor(s + v, device=d.device, dtype=d.dtype)\n",
    "    log_det_term = torch.sum(torch.log(eta + sPlusv))\n",
    "    log_sv_term = torch.log(sPlusv) * (sample_size - k)\n",
    "    trace_term = sample_covariance_trace / (sPlusv)\n",
    "    eta_term = torch.sum(d * eta / (eta + sPlusv)) / (sPlusv)\n",
    "\n",
    "    return sample_size * torch.log(torch.tensor(2 * torch.pi, device=d.device, dtype=d.dtype)) + log_det_term + log_sv_term + trace_term - eta_term\n",
    "\n",
    "# independent maximum likelihood estimation for autoFRK, using in selectBasis\n",
    "# check = none\n",
    "def indeMLE(\n",
    "    data: torch.Tensor,\n",
    "    Fk: torch.Tensor,\n",
    "    D: Optional[torch.Tensor] = None,\n",
    "    maxit: int = 50,\n",
    "    avgtol: float = 1e-6,\n",
    "    wSave: bool = False,\n",
    "    DfromLK: Optional[dict] = None,\n",
    "    vfixed: Optional[float] = None,\n",
    "    verbose: bool = True,\n",
    "    device: Optional[Union[torch.device, str]]='cpu'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    data = data.to(device)\n",
    "    Fk = Fk.to(device)\n",
    "\n",
    "    withNA = torch.isnan(data).any().item()\n",
    "\n",
    "    TT = data.shape[1]\n",
    "    empty = torch.isnan(data).all(dim=0)\n",
    "    notempty = (~empty).nonzero(as_tuple=True)[0]\n",
    "    if empty.any():\n",
    "        data = data[:, notempty]\n",
    "\n",
    "    del_rows = torch.isnan(data).all(dim=1).nonzero(as_tuple=True)[0]\n",
    "    pick = torch.arange(data.shape[0], device=device)\n",
    "\n",
    "    if D is None:\n",
    "        D = torch.eye(data.shape[0], device=device).to_sparse()\n",
    "\n",
    "    if not torch.allclose(D, torch.diag(torch.diagonal(D))):\n",
    "        D0 = D\n",
    "    else:\n",
    "        D0 = torch.diag(torch.diag(D)).to_sparse()\n",
    "\n",
    "    if withNA and len(del_rows) > 0:\n",
    "        pick = pick[~torch.isin(pick, del_rows)]\n",
    "        data = data[~torch.isin(torch.arange(data.shape[0], device=device), del_rows), :]\n",
    "        Fk = Fk[~torch.isin(torch.arange(Fk.shape[0], device=device), del_rows), :]\n",
    "        if not torch.allclose(D, torch.diag(torch.diagonal(D))):\n",
    "            D = D[~torch.isin(torch.arange(D.shape[0], device=device), del_rows)][:, ~torch.isin(torch.arange(D.shape[1], device=device), del_rows)]\n",
    "        else:\n",
    "            keep_mask = ~torch.isin(torch.arange(D.shape[0], device=device), del_rows)\n",
    "            full_diag = torch.zeros(D.shape[0], device=device)\n",
    "            full_diag[keep_mask] = torch.diagonal(D)[keep_mask]\n",
    "            D = torch.diag(full_diag)\n",
    "        withNA = torch.isnan(data).any().item()\n",
    "\n",
    "    N = data.shape[0]\n",
    "    K = Fk.shape[1]\n",
    "    Depsilon = D\n",
    "    is_diag = torch.allclose(D, torch.diag(torch.diagonal(D)))\n",
    "    mean_diag = torch.mean(torch.diagonal(D))\n",
    "    isimat = is_diag and torch.allclose(torch.diagonal(Depsilon), mean_diag.repeat(N), atol=1e-10)\n",
    "\n",
    "    if not withNA:\n",
    "        if isimat and DfromLK is None:\n",
    "            sigma = 0  # we cannot find `.Option$sigma_FRK` in the R code # need fix\n",
    "            out = cMLEimat(Fk, \n",
    "                           data, \n",
    "                           s=sigma, \n",
    "                           wSave=wSave\n",
    "                           )\n",
    "            if out['v'] is not None:\n",
    "                out['s'] = out['v'] if sigma == 0 else sigma\n",
    "                del out['v']\n",
    "            if wSave:\n",
    "                w = torch.zeros((K, TT), device=device)\n",
    "                w[:, notempty] = out['w']\n",
    "                out['w'] = w\n",
    "                out['pinfo'] = {'D': D0, \n",
    "                                'pick': pick\n",
    "                                }\n",
    "            return out\n",
    "        elif DfromLK is None:\n",
    "            out = cMLEsp(Fk, \n",
    "                         data, \n",
    "                         Depsilon, \n",
    "                         wSave\n",
    "                         )\n",
    "            if wSave:\n",
    "                w = torch.zeros((K, TT), device=device)\n",
    "                w[:, notempty] = out['w']\n",
    "                out['w'] = w\n",
    "                out['pinfo'] = {'D': D0, \n",
    "                                'pick': pick\n",
    "                                }\n",
    "            return out\n",
    "        else:\n",
    "            out = cMLElk(Fk, \n",
    "                         data, \n",
    "                         Depsilon, \n",
    "                         wSave, \n",
    "                         DfromLK, \n",
    "                         vfixed\n",
    "                         )\n",
    "            if wSave:\n",
    "                w = torch.zeros((K, TT), device=device)\n",
    "                w[:, notempty] = out['w']\n",
    "                out['w'] = w\n",
    "            return out\n",
    "    else:\n",
    "        out = EM0miss(Fk, \n",
    "                      data, \n",
    "                      Depsilon, \n",
    "                      maxit, \n",
    "                      avgtol, \n",
    "                      wSave,\n",
    "                      DfromLK=DfromLK, \n",
    "                      vfixed=vfixed, \n",
    "                      verbose=verbose\n",
    "                      )\n",
    "        if wSave:\n",
    "            w = torch.zeros((K, TT), device=device)\n",
    "            w[:, notempty] = out['w']\n",
    "            out['w'] = w\n",
    "            if DfromLK is None:\n",
    "                out['pinfo'] = {'D': D0,\n",
    "                                'pick': pick\n",
    "                                }\n",
    "        return out\n",
    "\n",
    "# convert dense tensor to sparse matrix, using in indeMLE\n",
    "# python 不需要，在 R 中僅作為節省記憶體的角色\n",
    "# def toSparseMatrix(\n",
    "#     mat: torch.Tensor, \n",
    "#     verbose: bool=False\n",
    "# ) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "#     if not torch.is_tensor(mat):\n",
    "#         warn_msg = f'Expected tensor, but got {type(mat)}'\n",
    "#         LOGGER.warning(warn_msg)\n",
    "#         mat = torch.tensor(mat)\n",
    "    \n",
    "#     if mat.is_sparse:\n",
    "#         if verbose:\n",
    "#             info_msg = f'The input is already a sparse tensor'\n",
    "#             LOGGER.info(info_msg)\n",
    "#         return mat\n",
    "\n",
    "#     if verbose:\n",
    "#         return mat.to_sparse()\n",
    "\n",
    "# using in indeMLE\n",
    "# check = ok\n",
    "def cMLEimat(\n",
    "    Fk: torch.Tensor,\n",
    "    data: torch.Tensor,\n",
    "    s: float,\n",
    "    wSave: bool = False,\n",
    "    S: Optional[torch.Tensor] = None,\n",
    "    onlylogLike: Optional[bool] = None,\n",
    "    device: Optional[Union[torch.device, str]]='cpu'\n",
    ") -> dict:\n",
    "\n",
    "    if onlylogLike is None:\n",
    "        onlylogLike = not wSave\n",
    "\n",
    "    data = data.to(device)\n",
    "    Fk = Fk.to(device)\n",
    "\n",
    "    num_columns = data.shape[1]\n",
    "    nrow_Fk, ncol_Fk = Fk.shape\n",
    "\n",
    "    projection = computeProjectionMatrix(Fk1=Fk, \n",
    "                                         Fk2=Fk, \n",
    "                                         data=data, \n",
    "                                         S=S, \n",
    "                                         device=device\n",
    "                                         )\n",
    "    inverse_square_root_matrix = projection[\"inverse_square_root_matrix\"]\n",
    "    matrix_JSJ = projection[\"matrix_JSJ\"]\n",
    "\n",
    "    sample_covariance_trace = torch.sum(data ** 2) / num_columns\n",
    "\n",
    "    likelihood_object = computeNegativeLikelihood(nrow_Fk=nrow_Fk,\n",
    "                                                  ncol_Fk=ncol_Fk,\n",
    "                                                  s=s,\n",
    "                                                  p=num_columns,\n",
    "                                                  matrix_JSJ=matrix_JSJ,\n",
    "                                                  sample_covariance_trace=sample_covariance_trace,\n",
    "                                                  device=device\n",
    "                                                  )\n",
    "\n",
    "    negative_log_likelihood = likelihood_object[\"negative_log_likelihood\"]\n",
    "\n",
    "    if onlylogLike:\n",
    "        return {\"negloglik\": negative_log_likelihood}\n",
    "\n",
    "    P = likelihood_object[\"P\"]\n",
    "    d_hat = likelihood_object[\"d_hat\"]\n",
    "    v = likelihood_object[\"v\"]\n",
    "\n",
    "    M = inverse_square_root_matrix @ P @ (P.T * d_hat[:, None]) @ inverse_square_root_matrix\n",
    "\n",
    "    if not wSave:\n",
    "        return {\"v\": v, \n",
    "                \"M\": M, \n",
    "                \"s\": s, \n",
    "                \"negloglik\": negative_log_likelihood\n",
    "                }\n",
    "\n",
    "    L = Fk @ ((torch.diag(torch.sqrt(d_hat)) @ P.T) @ inverse_square_root_matrix).T\n",
    "\n",
    "    if ncol_Fk > 2:\n",
    "        reduced_columns = torch.cat([\n",
    "            torch.tensor([0], device=device),\n",
    "            (d_hat[1:(ncol_Fk - 1)] > 0).nonzero(as_tuple=True)[0]\n",
    "        ])\n",
    "    else:\n",
    "        reduced_columns = torch.tensor([ncol_Fk - 1], device=device)\n",
    "\n",
    "    L = L[:, reduced_columns]\n",
    "\n",
    "    invD = torch.ones(nrow_Fk, device=device) / (s + v)\n",
    "    iDZ = invD[:, None] * data\n",
    "\n",
    "    right = L @ (torch.linalg.inv(torch.eye(L.shape[1], device=device) + L.T @ (invD[:, None] * L)) @ (L.T @ iDZ))\n",
    "\n",
    "    INVtZ = iDZ - invD[:, None] * right\n",
    "    etatt = M @ Fk.T @ INVtZ\n",
    "\n",
    "    GM = Fk @ M\n",
    "\n",
    "    diag_matrix = (s + v) * torch.eye(nrow_Fk, device=device)\n",
    "\n",
    "    V = M - GM.T @ invCz(R=diag_matrix,\n",
    "                         L=L, \n",
    "                         z=GM,\n",
    "                         device=device\n",
    "                         ).T\n",
    "\n",
    "    return {\"v\": v,\n",
    "            \"M\": M,\n",
    "            \"s\": s,\n",
    "            \"negloglik\": negative_log_likelihood,\n",
    "            \"w\": etatt,\n",
    "            \"V\": V\n",
    "            }\n",
    "\n",
    "# using in cMLEimat\n",
    "# check = ok\n",
    "def computeProjectionMatrix(\n",
    "    Fk1: torch.Tensor, \n",
    "    Fk2: torch.Tensor, \n",
    "    data: torch.Tensor, \n",
    "    S: torch.Tensor=None, \n",
    "    device: Optional[Union[torch.device, str]]='cpu'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Internal function: maximum likelihood estimate with the likelihood\n",
    "\n",
    "    Parameters:\n",
    "        Fk1 (torch.Tensor): (n, K) matrix\n",
    "        Fk2 (torch.Tensor): (n, K) matrix\n",
    "        data (torch.Tensor): (n, T) matrix\n",
    "        S (torch.Tensor or None): (n, n) matrix\n",
    "        device (str): \"cpu\" or \"cuda\"\n",
    " \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'inverse_square_root_matrix': torch.Tensor,\n",
    "            'matrix_JSJ': torch.Tensor\n",
    "        }\n",
    "    \"\"\"\n",
    "    Fk1 = Fk1.to(device)\n",
    "    Fk2 = Fk2.to(device)\n",
    "    data = data.to(device)\n",
    "    if S is not None:\n",
    "        S = S.to(device)\n",
    "\n",
    "    num_columns = data.shape[1]\n",
    "    inverse_square_root_matrix = getInverseSquareRootMatrix(A=Fk1, \n",
    "                                                            B=Fk2, \n",
    "                                                            device=device\n",
    "                                                            )\n",
    "    inverse_square_root_on_Fk2 = inverse_square_root_matrix @ Fk2.T\n",
    "\n",
    "    if S is None:\n",
    "        matrix_JSJ = (inverse_square_root_on_Fk2 @ data) @ (inverse_square_root_on_Fk2 @ data).T / num_columns\n",
    "    else:\n",
    "        matrix_JSJ = (inverse_square_root_on_Fk2 @ S) @ inverse_square_root_on_Fk2.T\n",
    "\n",
    "    matrix_JSJ = (matrix_JSJ + matrix_JSJ.T) / 2\n",
    "\n",
    "    return {\"inverse_square_root_matrix\": inverse_square_root_matrix,\n",
    "            \"matrix_JSJ\": matrix_JSJ\n",
    "            }\n",
    "\n",
    "# using in computeProjectionMatrix\n",
    "# check = ok\n",
    "def getInverseSquareRootMatrix(\n",
    "    A: torch.Tensor, \n",
    "    B: torch.Tensor, \n",
    "    device: Optional[Union[torch.device, str]]='cpu',\n",
    "    eps: float = 1e-10\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute inverse square root matrix of (A.T @ B), assuming it is symmetric.\n",
    "    \n",
    "    Parameters:\n",
    "        A: Tensor of shape (n, k)\n",
    "        B: Tensor of shape (n, k)\n",
    "        device: 'cpu' or 'cuda'\n",
    "        \n",
    "    Returns:\n",
    "        Inverse square root of (A.T @ B): Tensor of shape (k, k)\n",
    "    \"\"\"\n",
    "    A = A.to(device)\n",
    "    B = B.to(device)\n",
    "\n",
    "    mat = A.T @ B\n",
    "\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(mat)\n",
    "    eigvals_clamped = torch.clamp(eigenvalues, min=eps)\n",
    "    inv_sqrt_eigvals = torch.diag(eigvals_clamped.rsqrt())\n",
    "\n",
    "    return eigenvectors @ inv_sqrt_eigvals @ eigenvectors.T\n",
    "\n",
    "# using in cMLEimat\n",
    "# check = ok\n",
    "def invCz(\n",
    "    R: torch.Tensor, \n",
    "    L: torch.Tensor, \n",
    "    z: torch.Tensor, \n",
    "    device: Optional[Union[torch.device, str]]='cpu'\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "        R: (p x p) positive definite matrix\n",
    "        L: (p x K) matrix\n",
    "        z: (p,) vector or (1 x p) row matrix\n",
    "        device: 'cpu' or 'cuda', or torch.device\n",
    "\n",
    "    Returns:\n",
    "        (1 x p) tensor\n",
    "    \"\"\"\n",
    "\n",
    "    dtype = R.dtype\n",
    "    if R.dtype != torch.float64:\n",
    "        R = R.to(dtype=torch.float64)\n",
    "        L = L.to(dtype=torch.float64)\n",
    "        z = z.to(dtype=torch.float64)\n",
    "\n",
    "    if z.dim() == 1:\n",
    "        z = z.unsqueeze(1)\n",
    "\n",
    "    K = L.shape[1]\n",
    "    iR = torch.linalg.pinv(R)\n",
    "    iRZ = iR @ z\n",
    "    right = L @ torch.linalg.inv(torch.eye(K, device=R.device, dtype=torch.float64) + (L.T @ iR @ L)) @ (L.T @ iRZ) \n",
    "    result = iRZ - iR @ right\n",
    "\n",
    "    return result.T.to(dtype=dtype)\n",
    "\n",
    "# using in indeMLE\n",
    "# check = ok, but have some problem\n",
    "def EM0miss(\n",
    "    Fk: torch.Tensor, \n",
    "    data: torch.Tensor, \n",
    "    Depsilon: torch.Tensor, \n",
    "    maxit: int=100, \n",
    "    avgtol: float=1e-4, \n",
    "    wSave: bool=False, \n",
    "    DfromLK: dict=None,\n",
    "    vfixed: float=None,\n",
    "    verbose: bool=True,\n",
    "    device: Optional[Union[torch.device, str]] = 'cpu'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Fk = Fk.to(device)\n",
    "    data = data.to(device)\n",
    "    Depsilon = Depsilon.to(device)\n",
    "\n",
    "    O = ~torch.isnan(data)\n",
    "    TT = data.shape[1]\n",
    "    ncol_Fk = Fk.shape[1]\n",
    "\n",
    "    ziDz = torch.full((TT,), float('nan'), device=device)\n",
    "    ziDB = torch.full((TT, ncol_Fk), float('nan'), device=device)\n",
    "    db = {}\n",
    "    D = Depsilon\n",
    "    iD = torch.linalg.inv(D)\n",
    "    diagD = isDiagonal(D)\n",
    "\n",
    "    if DfromLK is not None:\n",
    "        pick = DfromLK.get(\"pick\", None)\n",
    "        weights = DfromLK[\"weights\"].clone().detach()\n",
    "        if pick is None:\n",
    "            pick = torch.arange(len(weights), device=device)\n",
    "        else:\n",
    "            pick = torch.tensor(pick, dtype=torch.long, device=device)\n",
    "        weight = weights[pick]\n",
    "\n",
    "        if not torch.is_tensor(DfromLK[\"wX\"]):\n",
    "            DfromLK[\"wX\"] = torch.tensor(DfromLK[\"wX\"], device=device)\n",
    "        DfromLK[\"wX\"] = DfromLK[\"wX\"][pick, :].clone().detach()\n",
    "\n",
    "        if not torch.is_tensor(DfromLK[\"Q\"]):\n",
    "            DfromLK[\"Q\"] = torch.tensor(DfromLK[\"Q\"], device=device)\n",
    "\n",
    "        wwX = torch.diag(torch.sqrt(weight)) @ DfromLK[\"wX\"]\n",
    "        lQ = DfromLK[\"lambda\"] * DfromLK[\"Q\"].clone().detach()\n",
    "\n",
    "    for tt in range(TT):\n",
    "        if DfromLK is not None:\n",
    "            obs_idx = O[:, tt].bool()\n",
    "            iDt = None\n",
    "            if obs_idx.sum() == O.shape[0]:\n",
    "                wXiG = wwX @ torch.linalg.inv(DfromLK[\"G\"].to(device=device))\n",
    "            else:\n",
    "                wX_obs = DfromLK[\"wX\"][obs_idx, :].to(device)\n",
    "                G = wX_obs.T @ wX_obs + lQ.to(device)\n",
    "                wXiG = wwX[obs_idx, :] @ torch.linalg.inv(G)\n",
    "\n",
    "            Bt = Fk[obs_idx, :].to(device)\n",
    "            if Bt.ndim == 1:\n",
    "                Bt = Bt.unsqueeze(0)\n",
    "\n",
    "            iDBt = weight[obs_idx].unsqueeze(1) * Bt - wXiG @ (wwX[obs_idx, :].T @ Bt)\n",
    "            zt = data[obs_idx, tt].to(device=device)\n",
    "            ziDz[tt] = torch.sum(zt * (weight[obs_idx] * zt - wXiG @ (wwX[obs_idx, :].T @ zt)))\n",
    "            ziDB[tt, :] = (zt @ iDBt).squeeze()\n",
    "            BiDBt = Bt.T @ iDBt\n",
    "\n",
    "        else:\n",
    "            if not diagD:\n",
    "                iDt = torch.linalg.inv(D[obs_idx][:, obs_idx].to(device))\n",
    "            else:\n",
    "                iDt = iD[obs_idx][:, obs_idx].to(device)\n",
    "\n",
    "            Bt = Fk[obs_idx, :].to(device)\n",
    "            if Bt.ndim == 1:\n",
    "                Bt = Bt.unsqueeze(0)\n",
    "\n",
    "            iDBt = iDt @ Bt\n",
    "            zt = data[obs_idx, tt]\n",
    "            ziDz[tt] = torch.sum(zt * (iDt @ zt))\n",
    "            ziDB[tt, :] = (zt @ iDBt).squeeze()\n",
    "            BiDBt = Bt.T @ iDBt\n",
    "\n",
    "        db[tt] = {\"iDBt\": iDBt,\n",
    "                  \"zt\": zt,\n",
    "                  \"BiDBt\": BiDBt\n",
    "                  }\n",
    "\n",
    "    del iDt, Bt, iDBt, zt, BiDBt\n",
    "    _ = gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    dif = float(\"inf\")\n",
    "    cnt = 0\n",
    "    Z0 = data.clone()\n",
    "    Z0[torch.isnan(Z0)] = 0\n",
    "    old = cMLEimat(Fk=Fk, \n",
    "                   data=Z0, \n",
    "                   s=0, \n",
    "                   wSave=True,\n",
    "                   device=device\n",
    "                   )\n",
    "    if vfixed is None:\n",
    "        old[\"s\"] = old[\"v\"]\n",
    "    else:\n",
    "        old[\"s\"] = vfixed.to(old[\"v\"].device)\n",
    "    old[\"M\"] = convertToPositiveDefinite(mat=old[\"M\"],\n",
    "                                         device=device\n",
    "                                         )\n",
    "    Ptt1 = old[\"M\"]\n",
    "\n",
    "    while (dif > (avgtol * (100 * (ncol_Fk ** 2)))) and (cnt < maxit):\n",
    "        etatt = torch.zeros((ncol_Fk, TT), device=device)\n",
    "        sumPtt = torch.zeros((ncol_Fk, ncol_Fk), device=device)\n",
    "        s1 = torch.zeros(TT, device=device)\n",
    "\n",
    "        for tt in range(TT):\n",
    "            iDBt = db[tt][\"iDBt\"].to(device)\n",
    "            zt = db[tt][\"zt\"].to(device)\n",
    "            BiDBt = db[tt][\"BiDBt\"].to(device)\n",
    "            \n",
    "            ginv_Ptt1 = torch.linalg.pinv(convertToPositiveDefinite(Ptt1))\n",
    "            iP = convertToPositiveDefinite(ginv_Ptt1 + BiDBt / old[\"s\"])\n",
    "            Ptt = torch.linalg.inv(iP)  # will broken\n",
    "            Gt = (Ptt @ iDBt.T) / old[\"s\"]\n",
    "            eta = Gt @ zt\n",
    "            s1kk = torch.diagonal(BiDBt @ (eta.unsqueeze(1) @ eta.unsqueeze(0) + Ptt))\n",
    "            \n",
    "            sumPtt += Ptt\n",
    "            etatt[:, tt] = eta\n",
    "            s1[tt] = torch.sum(s1kk)\n",
    "\n",
    "        if vfixed is None:\n",
    "            s = torch.max(\n",
    "                (torch.sum(ziDz) - 2 * torch.sum(ziDB * etatt.T) + torch.sum(s1)) / torch.sum(O),\n",
    "                torch.tensor(1e-8, dtype=ziDz.dtype, device=ziDz.device)\n",
    "            )\n",
    "            new = {\"M\": (etatt @ etatt.T + sumPtt) / TT,\n",
    "                   \"s\": s,\n",
    "                   }\n",
    "        else:\n",
    "            new = {\"M\": (etatt @ etatt.T + sumPtt) / TT,\n",
    "                   \"s\": vfixed.to(device),\n",
    "                   }\n",
    "\n",
    "        new[\"M\"] = (new[\"M\"] + new[\"M\"].T) / 2\n",
    "        dif = torch.sum(torch.abs(new[\"M\"] - old[\"M\"])) + torch.abs(new[\"s\"] - old[\"s\"])\n",
    "        cnt += 1\n",
    "        old = new\n",
    "        Ptt1 = old[\"M\"]\n",
    "\n",
    "    if verbose:\n",
    "        info_msg = f'Number of iteration: {cnt}'\n",
    "        LOGGER.info(info_msg)\n",
    "        \n",
    "    n2loglik = computeLikelihood(data=data,\n",
    "                                 Fk=Fk,\n",
    "                                 M=new[\"M\"],\n",
    "                                 s=new[\"s\"],\n",
    "                                 Depsilon=Depsilon,\n",
    "                                 device=device\n",
    "                                 )\n",
    "\n",
    "    if not wSave:\n",
    "        return {\n",
    "            \"M\": new[\"M\"],\n",
    "            \"s\": new[\"s\"],\n",
    "            \"negloglik\": n2loglik\n",
    "        }\n",
    "\n",
    "    elif DfromLK is not None:\n",
    "        out = {\n",
    "            \"M\": new[\"M\"],\n",
    "            \"s\": new[\"s\"],\n",
    "            \"negloglik\": n2loglik,\n",
    "            \"w\": etatt,\n",
    "            \"V\": new[\"M\"] - (etatt @ etatt.T) / TT\n",
    "        }\n",
    "\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(new[\"M\"])\n",
    "        L = Fk @ eigenvectors @ torch.diag(torch.sqrt(torch.clamp(eigenvalues, min=0.0)))\n",
    "\n",
    "        weight = DfromLK[\"weights\"][pick]\n",
    "        wlk = torch.full((lQ.shape[0], TT), float(\"nan\"), device=device)\n",
    "\n",
    "        for tt in range(TT):\n",
    "            obs_idx = O[:, tt].bool()\n",
    "            if torch.sum(obs_idx) == O.shape[0]:\n",
    "                wXiG = wwX @ torch.linalg.solve(DfromLK[\"G\"], torch.eye(DfromLK[\"G\"].shape[0], device=device))\n",
    "            else:\n",
    "                wX_tt = DfromLK[\"wX\"][obs_idx]\n",
    "                G = wX_tt.T @ wX_tt + lQ\n",
    "                wXiG = wwX[obs_idx] @ torch.linalg.solve(G, torch.eye(G.shape[0], device=device))\n",
    "\n",
    "            dat = data[obs_idx, tt]\n",
    "            Lt = L[obs_idx]\n",
    "            iDL = weight[obs_idx].unsqueeze(1) * Lt - wXiG @ (wwX[obs_idx].T @ Lt)\n",
    "            itmp = torch.linalg.solve(\n",
    "                torch.eye(L.shape[1], device=device) + (Lt.T @ iDL) / out[\"s\"],\n",
    "                torch.eye(L.shape[1], device=device)\n",
    "            )\n",
    "            iiLiD = itmp @ (iDL.T / out[\"s\"])\n",
    "            wlk[:, tt] = (wXiG.T @ dat - wXiG.T @ Lt @ (iiLiD @ dat)).squeeze()\n",
    "\n",
    "        out[\"pinfo\"] = {\n",
    "            \"wlk\": wlk, \n",
    "            \"pick\": pick\n",
    "        }\n",
    "        out[\"missing\"] = {\n",
    "            \"miss\": 1 - O, \n",
    "            \"maxit\": maxit, \n",
    "            \"avgtol\": avgtol\n",
    "        }\n",
    "        return out\n",
    "\n",
    "    else:\n",
    "        out = {\n",
    "            \"M\": new[\"M\"],\n",
    "            \"s\": new[\"s\"],\n",
    "            \"negloglik\": n2loglik,\n",
    "            \"w\": etatt,\n",
    "            \"V\": new[\"M\"] - (etatt @ etatt.T) / TT\n",
    "        }\n",
    "        out[\"missing\"] = {\n",
    "            \"miss\": 1 - O, \n",
    "            \"maxit\": maxit, \n",
    "            \"avgtol\": avgtol\n",
    "        }\n",
    "        return out\n",
    "\n",
    "# using in EM0miss\n",
    "# check = ok\n",
    "def isDiagonal(\n",
    "    tensor: torch.Tensor,\n",
    "    tol=1e-10\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Internal function: check if a numeric-like object is diagonal\n",
    "\n",
    "    Parameters:\n",
    "        tensor:\n",
    "        tol:\n",
    "    \n",
    "    Return:\n",
    "        bool: \n",
    "    \"\"\"\n",
    "    if tensor.numel() == 1:\n",
    "        return True\n",
    "\n",
    "    if tensor.ndim != 2 or tensor.shape[0] != tensor.shape[1]:\n",
    "        return False\n",
    "\n",
    "    diag = torch.diag(torch.diagonal(tensor))\n",
    "    return torch.allclose(tensor, diag, atol=tol)\n",
    "\n",
    "# using in EM0miss\n",
    "# check = ok\n",
    "def convertToPositiveDefinite(\n",
    "    mat: torch.Tensor, \n",
    "    device: Optional[Union[torch.device, str]] = 'cpu'\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Internal function: convert a matrix to positive definite\n",
    "\n",
    "    Parameters:\n",
    "        mat (torch.Tensor): Input 2D matrix (square, symmetric or not).\n",
    "        device (torch.device): Device to perform computation on (\"cpu\" or \"cuda\").\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A positive-definite version of the input matrix.\n",
    "    \"\"\"\n",
    "    mat = mat.to(device)\n",
    "\n",
    "    # Ensure symmetry\n",
    "    if not torch.allclose(mat, mat.T, atol=1e-10):\n",
    "        mat = (mat + mat.T) / 2\n",
    "\n",
    "    try:\n",
    "        # Compute eigenvalues only\n",
    "        eigenvalues = torch.linalg.eigvalsh(mat)\n",
    "        min_eigenvalue = torch.min(eigenvalues).item()\n",
    "    except RuntimeError:\n",
    "        # Fallback in case of numerical error\n",
    "        mat = (mat + mat.T) / 2\n",
    "        eigenvalues = torch.linalg.eigvalsh(mat)\n",
    "        min_eigenvalue = torch.min(eigenvalues).item()\n",
    "\n",
    "    if min_eigenvalue <= 0:\n",
    "        adjustment = abs(min_eigenvalue) + 1e-6\n",
    "        mat = mat + torch.eye(mat.shape[0], device=device) * adjustment\n",
    "\n",
    "    return mat\n",
    "\n",
    "# using in EM0miss\n",
    "# check = ok\n",
    "def computeLikelihood(\n",
    "    data: torch.Tensor,\n",
    "    Fk: torch.Tensor,\n",
    "    M: torch.Tensor,\n",
    "    s: float,\n",
    "    Depsilon: torch.Tensor,\n",
    "    device: Optional[Union[torch.device, str]] = 'cpu'\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute negative log-likelihood (-2 * log(likelihood)).\n",
    "\n",
    "    Parameters:\n",
    "        data (n x T): Observation matrix with possible NaNs.\n",
    "        Fk (n x K): Basis function matrix.\n",
    "        M (K x K): Symmetric matrix.\n",
    "        s (float): Scalar multiplier.\n",
    "        Depsilon (n x n): Diagonal matrix.\n",
    "        device: CPU or GPU.\n",
    "\n",
    "    Returns:\n",
    "        float: Negative log-likelihood value.\n",
    "    \"\"\"\n",
    "    data = data.to(device)\n",
    "    Fk = Fk.to(device)\n",
    "    M = M.to(device)\n",
    "    Depsilon = Depsilon.to(device)\n",
    "\n",
    "    non_missing_points_matrix = ~torch.isnan(data)\n",
    "    num_columns = data.shape[1]\n",
    "\n",
    "    n2loglik = non_missing_points_matrix.sum() * torch.log(torch.tensor(2 * torch.pi, device=device))\n",
    "    R = s * Depsilon\n",
    "    eg = eigenDecompose(M,\n",
    "                        device=device\n",
    "                        )\n",
    "    K = Fk.shape[1]\n",
    "    L = Fk @ eg[\"vector\"] @ torch.diag(torch.sqrt(torch.clamp(eg[\"value\"], min=0.0))) @ eg[\"vector\"].T\n",
    "    \n",
    "    for t in range(num_columns):\n",
    "        mask = non_missing_points_matrix[:, t]\n",
    "        zt = data[mask, t]\n",
    "\n",
    "        # skip all-missing column\n",
    "        if zt.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        Rt = R[mask][:, mask]\n",
    "        Lt = L[mask]\n",
    "\n",
    "        log_det = calculateLogDeterminant(Rt, \n",
    "                                          Lt, \n",
    "                                          K, \n",
    "                                          device=device\n",
    "                                          )\n",
    "        inv_cz_val = invCz(Rt, \n",
    "                           Lt, \n",
    "                           zt, \n",
    "                           device=device\n",
    "                           )\n",
    "        n2loglik += log_det + torch.sum(zt * inv_cz_val)\n",
    "\n",
    "    return n2loglik.item()\n",
    "\n",
    "# using in computeLikelihood\n",
    "# check = ok\n",
    "def eigenDecompose(\n",
    "    matrix: torch.Tensor,\n",
    "    device: Optional[Union[torch.device, str]] = 'cpu'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Internal function: Eigen-decompose a matrix\n",
    "\n",
    "    Parameters:\n",
    "        matrix (torch.Tensor): (K x K) symmetric matrix\n",
    "        device (str or torch.device): computation device\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            'value': (K,) tensor of eigenvalues\n",
    "            'vector': (K x K) tensor of eigenvectors (columns)\n",
    "    \"\"\"\n",
    "    matrix = matrix.to(device)\n",
    "\n",
    "    # Use symmetric eigendecomposition\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(matrix)\n",
    "\n",
    "    return {\n",
    "        'value': eigenvalues,\n",
    "        'vector': eigenvectors\n",
    "    }\n",
    "\n",
    "# using in computeLikelihood\n",
    "# check = ok\n",
    "def calculateLogDeterminant(\n",
    "    R: torch.Tensor,\n",
    "    L: torch.Tensor,\n",
    "    K: int=None,\n",
    "    device: Optional[Union[str, torch.device]] = 'cpu'\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Internal function: calculate the log determinant for the likelihood use.\n",
    "\n",
    "    Parameters:\n",
    "        R (torch.Tensor): (p x p) positive-definite matrix\n",
    "        L (torch.Tensor): (p x K) matrix\n",
    "        K (int): A numeric\n",
    "        device (str or torch.device): computation device\n",
    "\n",
    "    Returns:\n",
    "        float: log-determinant value\n",
    "    \"\"\"\n",
    "    R = R.to(device)\n",
    "    L = L.to(device)\n",
    "\n",
    "    if K is None:\n",
    "        K = L.shape[1]\n",
    "\n",
    "    first_part_determinant = torch.logdet(torch.eye(K, device=device) + L.T @ torch.linalg.solve(R, L))\n",
    "    second_part_determinant = torch.logdet(R)\n",
    "\n",
    "    return (first_part_determinant + second_part_determinant).item()\n",
    "\n",
    "# using in indeMLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fd914a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cMLE(\n",
    "    Fk: torch.Tensor,\n",
    "    num_columns: int,\n",
    "    sample_covariance_trace: float,\n",
    "    inverse_square_root_matrix: torch.Tensor,\n",
    "    matrix_JSJ: torch.Tensor,\n",
    "    s: float = 0,\n",
    "    ldet: float = 0,\n",
    "    wSave: bool = False,\n",
    "    onlylogLike: bool = None,\n",
    "    vfixed: float = None,\n",
    "    device: Optional[Union[torch.device, str]] = 'cpu'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Internal function: maximum likelihood estimate with the likelihood\n",
    "\n",
    "    Parameters:\n",
    "        Fk: (N, K) torch.Tensor, basis functions.\n",
    "        num_columns: (int) Number of columns in the data.\n",
    "        sample_covariance_trace: (float) Trace of the sample covariance matrix.\n",
    "        inverse_square_root_matrix: (N, K) torch.Tensor, inverse square root matrix.\n",
    "        matrix_JSJ: (K, K) torch.Tensor, covariance-like matrix.\n",
    "        s: (float) Effective sample size, default is 0.\n",
    "        ldet: (float) Log determinant of the transformation matrix, default is 0.\n",
    "        wSave: (bool) Whether to save the L matrix, default is False.\n",
    "        onlylogLike: (bool) If True, only return the negative log likelihood.\n",
    "        vfixed: (float, optional) Fixed noise variance, if provided.\n",
    "        device: (str) 'cpu' or 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'v': (float) Estimated noise variance,\n",
    "            'M': (torch.Tensor) Matrix M,\n",
    "            's': (int) Effective sample size,\n",
    "            'negloglik': (float) Negative log likelihood,\n",
    "            'L': (torch.Tensor, optional) L matrix if wSave is True.\n",
    "        }\n",
    "    \"\"\"\n",
    "    nrow_Fk = Fk.shape[0]\n",
    "\n",
    "    likelihood_object = computeNegativeLikelihood(\n",
    "        nrow_Fk=nrow_Fk,\n",
    "        ncol_Fk=Fk.shape[1],\n",
    "        s=s,\n",
    "        p=num_columns,\n",
    "        matrix_JSJ=matrix_JSJ,\n",
    "        sample_covariance_trace=sample_covariance_trace,\n",
    "        vfixed=vfixed,\n",
    "        ldet=ldet,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    negative_log_likelihood = likelihood_object['negative_log_likelihood']\n",
    "\n",
    "    if onlylogLike:\n",
    "        return {'negloglik': negative_log_likelihood}\n",
    "\n",
    "    P = likelihood_object['P']\n",
    "    d_hat = likelihood_object['d_hat']\n",
    "    v = likelihood_object['v']\n",
    "    M = inverse_square_root_matrix @ P @ (torch.diag(d_hat) @ P.T) @ inverse_square_root_matrix\n",
    "\n",
    "    if not wSave:\n",
    "        L = None\n",
    "    elif d_hat[0] != 0:\n",
    "        L = Fk @ ((torch.diag(torch.sqrt(d_hat)) @ P.T) @ inverse_square_root_matrix)\n",
    "        L = L[:, d_hat > 0]\n",
    "    else:\n",
    "        L = torch.zeros((nrow_Fk, 1), dtype=Fk.dtype, device=Fk.device)\n",
    "\n",
    "    return {'v': v,\n",
    "            'M': M,\n",
    "            's': s,\n",
    "            'negloglik': negative_log_likelihood,\n",
    "            'L': L\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "099943aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrow_Fk = 5\n",
      "negative_log_likelihood = 59.230576791600257\n",
      "P[0,0] = -0.37883572409221677\n",
      "P[0,1] = -0.83055212202396778\n",
      "P[0,2] = 0.40824829056961093\n",
      "P[1,0] = -0.5584200704187291\n",
      "P[1,1] = -0.14663000485615038\n",
      "P[1,2] = -0.81649658090492128\n",
      "P[2,0] = -0.73800441672463435\n",
      "P[2,1] = 0.537292112614143\n",
      "P[2,2] = 0.40824829040372412\n",
      "d_hat[0] = 14.756686031642779\n",
      "d_hat[1] = 0\n",
      "d_hat[2] = 0\n",
      "v = 1.9819961270047779\n",
      "M[0,0] = 4100994763.8106313\n",
      "M[0,1] = -8199995663.3397942\n",
      "M[0,2] = 4099235889.5982118\n",
      "M[1,0] = -8199995663.3397932\n",
      "M[1,1] = 16396004567.51432\n",
      "M[1,2] = -8196478770.0627089\n",
      "M[2,0] = 4099235889.5982113\n",
      "M[2,1] = -8196478770.0627098\n",
      "M[2,2] = 4097477769.7487383\n",
      "L[0,0] = -5220.4286690646959\n",
      "L[1,0] = -20611.683800526738\n",
      "L[2,0] = -36002.938931988778\n",
      "L[3,0] = -51394.194063450828\n",
      "L[4,0] = -66785.449194912857\n",
      "s = 0\n",
      "negloglik = 59.230576791600257\n"
     ]
    }
   ],
   "source": [
    "# ==== 固定測試資料 ====\n",
    "import torch, numpy as np\n",
    "device = \"cpu\"; dtype = torch.float64\n",
    "\n",
    "Fk = torch.tensor([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.4, 0.5, 0.6],\n",
    "    [0.7, 0.8, 0.9],\n",
    "    [1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5]\n",
    "], dtype=dtype, device=device)\n",
    "\n",
    "Z0 = torch.tensor([\n",
    "    [1, 0, 2],\n",
    "    [2, 3, 0],\n",
    "    [0, 1, 4],\n",
    "    [3, 2, 1],\n",
    "    [4, 3, 0]\n",
    "], dtype=dtype, device=device)\n",
    "\n",
    "# ==== 列印工具（逐元素，以 \"name[i]\" / \"name[i,j]\" = value 格式）====\n",
    "def print_scalar(name, x, precision=17):\n",
    "    if isinstance(x, torch.Tensor): x = x.item()\n",
    "    print(f\"{name} = {x:.{precision}g}\")\n",
    "\n",
    "def print_vector(name, t, precision=17):\n",
    "    a = t.detach().cpu().numpy() if isinstance(t, torch.Tensor) else np.asarray(t)\n",
    "    a = a.reshape(-1)\n",
    "    for i, v in enumerate(a):\n",
    "        print(f\"{name}[{i}] = {v:.{precision}g}\")\n",
    "\n",
    "def print_matrix(name, t, precision=17):\n",
    "    if t is None:\n",
    "        print(f\"{name} = NULL\"); return\n",
    "    A = t.detach().cpu().numpy() if isinstance(t, torch.Tensor) else np.asarray(t)\n",
    "    R, C = A.shape\n",
    "    for i in range(R):\n",
    "        for j in range(C):\n",
    "            print(f\"{name}[{i},{j}] = {A[i,j]:.{precision}g}\")\n",
    "\n",
    "# ==== cMLE 相關步驟 ====\n",
    "proj = computeProjectionMatrix(Fk, Fk, Z0, device=device)\n",
    "sample_cov_trace = float(torch.sum(Z0**2).item() / Z0.shape[1])\n",
    "\n",
    "# 先拿 likelihood 物件（為了取 P、d_hat、v、negloglik）\n",
    "lik = computeNegativeLikelihood(\n",
    "    nrow_Fk=Fk.shape[0],\n",
    "    ncol_Fk=Fk.shape[1],\n",
    "    s=0, p=Z0.shape[1],\n",
    "    matrix_JSJ=proj['matrix_JSJ'],\n",
    "    sample_covariance_trace=sample_cov_trace,\n",
    "    vfixed=None, ldet=0.0, device=device\n",
    ")\n",
    "\n",
    "# 用 cMLE 得到 M、L（就是你要檢查的重點）\n",
    "out = cMLE(\n",
    "    Fk=Fk,\n",
    "    num_columns=Z0.shape[1],\n",
    "    sample_covariance_trace=sample_cov_trace,\n",
    "    inverse_square_root_matrix=proj['inverse_square_root_matrix'],\n",
    "    matrix_JSJ=proj['matrix_JSJ'],\n",
    "    s=0, ldet=0.0, wSave=True, onlylogLike=False, device=device\n",
    ")\n",
    "\n",
    "# ==== 逐行列印：完整覆蓋 R 版左側與 return 的元素 ====\n",
    "print_scalar(\"nrow_Fk\", Fk.shape[0])\n",
    "\n",
    "print_scalar(\"negative_log_likelihood\", lik['negative_log_likelihood'])\n",
    "print_matrix(\"P\", lik['P'])\n",
    "print_vector(\"d_hat\", lik['d_hat'])\n",
    "print_scalar(\"v\", lik['v'])\n",
    "\n",
    "print_matrix(\"M\", out['M'])\n",
    "print_matrix(\"L\", out['L'])   # 若 None 會印成 NULL\n",
    "print_scalar(\"s\", 0)\n",
    "print_scalar(\"negloglik\", lik['negative_log_likelihood'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b70d9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_py[0,0] = -0.37883573366421197\n",
      "P_py[0,1] = -0.8305520162969896\n",
      "P_py[0,2] = 0.40824849678119213\n",
      "P_py[1,0] = -0.55842004510022214\n",
      "P_py[1,1] = -0.14663022080531096\n",
      "P_py[1,2] = -0.81649655943969024\n",
      "P_py[2,0] = -0.7380044309686522\n",
      "P_py[2,1] = 0.53729221711429098\n",
      "P_py[2,2] = 0.40824812712252001\n",
      "d_hat_py[0] = 14.756685964424952\n",
      "d_hat_py[1] = 0\n",
      "d_hat_py[2] = 0\n",
      "v_py = 1.9819961404483433\n",
      "negative_log_likelihood_py = 59.230576863356646\n",
      "M_from_cMLE_raw[0,0] = 1.3642586799576226\n",
      "M_from_cMLE_raw[0,1] = 3.5303392486700833\n",
      "M_from_cMLE_raw[0,2] = -6.3243891792387847\n",
      "M_from_cMLE_raw[1,0] = 3.5303392531547355\n",
      "M_from_cMLE_raw[1,1] = 9.1355806492093503\n",
      "M_from_cMLE_raw[1,2] = -16.36584005826959\n",
      "M_from_cMLE_raw[2,0] = -6.3243891768574061\n",
      "M_from_cMLE_raw[2,1] = -16.365840028932933\n",
      "M_from_cMLE_raw[2,2] = 29.318412350417312\n",
      "L_from_cMLE_raw[0,0] = 0.11680148462319535\n",
      "L_from_cMLE_raw[1,0] = 0.46720593849278141\n",
      "L_from_cMLE_raw[2,0] = 0.81761039236236743\n",
      "L_from_cMLE_raw[3,0] = 1.1680148462319535\n",
      "L_from_cMLE_raw[4,0] = 1.5184193001015396\n",
      "s_py = 0\n",
      "negloglik_py = 59.230576863356646\n",
      "v_py_out = 1.9819961404483433\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "dtype = torch.float64; device = \"cpu\"\n",
    "\n",
    "# ---- R 那邊印出來的 J、JSJ（照你貼的數值）----\n",
    "J_R = torch.tensor([\n",
    "    [ 5998983.7193301283 , -11997959.333595194,  5998976.5695005218 ],\n",
    "    [-11997959.333595185 ,  23995919.412441969, -11997959.753569832 ],\n",
    "    [ 5998976.5695005143 , -11997959.753569825,  5998982.8793871226 ],\n",
    "], dtype=dtype, device=device)\n",
    "\n",
    "JSJ_R = torch.tensor([\n",
    "    [3.3413369999902756 , 3.7068460514642916 , 4.0723548485718872 ],\n",
    "    [3.7068460514642916 , 5.2489414732442201 , 6.791037462524776  ],\n",
    "    [4.0723548485718872 , 6.791037462524776  , 9.5097214658459919 ],\n",
    "], dtype=dtype, device=device)\n",
    "\n",
    "# ---- 與 R 相同的 Fk、Z0 ----\n",
    "Fk = torch.tensor([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.4, 0.5, 0.6],\n",
    "    [0.7, 0.8, 0.9],\n",
    "    [1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5]\n",
    "], dtype=dtype, device=device)\n",
    "\n",
    "Z0 = torch.tensor([\n",
    "    [1, 0, 2],\n",
    "    [2, 3, 0],\n",
    "    [0, 1, 4],\n",
    "    [3, 2, 1],\n",
    "    [4, 3, 0]\n",
    "], dtype=dtype, device=device)\n",
    "\n",
    "# ---- 列印工具（0-based）----\n",
    "def print_scalar(name, x, precision=17):\n",
    "    if isinstance(x, torch.Tensor): x = x.item()\n",
    "    print(f\"{name} = {x:.{precision}g}\")\n",
    "def print_vector(name, t, precision=17):\n",
    "    a = t.detach().cpu().numpy().reshape(-1)\n",
    "    for i, v in enumerate(a):\n",
    "        print(f\"{name}[{i}] = {v:.{precision}g}\")\n",
    "def print_matrix(name, t, precision=17):\n",
    "    A = t.detach().cpu().numpy()\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            print(f\"{name}[{i},{j}] = {A[i,j]:.{precision}g}\")\n",
    "\n",
    "# ---- 跟 R 相同的 sample_cov_trace ----\n",
    "sample_cov_trace = float(torch.sum(Z0**2) / Z0.shape[1])\n",
    "\n",
    "# ---- 用你現有的 computeNegativeLikelihood（不改）----\n",
    "lik = computeNegativeLikelihood(\n",
    "    nrow_Fk=Fk.shape[0],\n",
    "    ncol_Fk=Fk.shape[1],\n",
    "    s=0, p=Z0.shape[1],\n",
    "    matrix_JSJ=JSJ_R,\n",
    "    sample_covariance_trace=sample_cov_trace,\n",
    "    vfixed=None, ldet=0.0, device=device\n",
    ")\n",
    "print_matrix(\"P_py\", lik['P'])\n",
    "print_vector(\"d_hat_py\", lik['d_hat'])\n",
    "print_scalar(\"v_py\", lik['v'])\n",
    "print_scalar(\"negative_log_likelihood_py\", lik['negative_log_likelihood'])\n",
    "\n",
    "# ---- 用「原始、不改動」的 cMLE 跑（只換輸入成 R 的 J/JSJ）----\n",
    "out_raw = cMLE(\n",
    "    Fk=Fk,\n",
    "    num_columns=Z0.shape[1],\n",
    "    sample_covariance_trace=sample_cov_trace,\n",
    "    inverse_square_root_matrix=J_R,\n",
    "    matrix_JSJ=JSJ_R,\n",
    "    s=0, ldet=0.0, wSave=True, onlylogLike=False, device=device\n",
    ")\n",
    "print_matrix(\"M_from_cMLE_raw\", out_raw['M'])\n",
    "print_matrix(\"L_from_cMLE_raw\", out_raw['L'])\n",
    "print_scalar(\"s_py\", out_raw['s'])\n",
    "print_scalar(\"negloglik_py\", out_raw['negloglik'])\n",
    "print_scalar(\"v_py_out\", out_raw['v'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28871599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_diag_check[0,0] = 1.3642586799576226\n",
      "M_diag_check[0,1] = 3.5303392486700833\n",
      "M_diag_check[0,2] = -6.3243891792387847\n",
      "M_diag_check[1,0] = 3.5303392531547355\n",
      "M_diag_check[1,1] = 9.1355806492093503\n",
      "M_diag_check[1,2] = -16.36584005826959\n",
      "M_diag_check[2,0] = -6.3243891768574061\n",
      "M_diag_check[2,1] = -16.365840028932933\n",
      "M_diag_check[2,2] = 29.318412350417312\n",
      "L_diag_check[0,0] = 0.11680148462319535\n",
      "L_diag_check[1,0] = 0.46720593849278141\n",
      "L_diag_check[2,0] = 0.81761039236236743\n",
      "L_diag_check[3,0] = 1.1680148462319535\n",
      "L_diag_check[4,0] = 1.5184193001015396\n"
     ]
    }
   ],
   "source": [
    "# ========= 對照驗證：函式外用 diag 寫法重算 M / L =========\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 列印工具（0-based）\n",
    "def print_matrix(name, t, precision=17):\n",
    "    A = t.detach().cpu().numpy()\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            print(f\"{name}[{i},{j}] = {A[i,j]:.{precision}g}\")\n",
    "\n",
    "# 1) 取得需要的變數：\n",
    "# - P、d_hat 來自你剛跑完的 likelihood 物件（lik）\n",
    "# - J 請用你「想要對齊 R」的那個 inverse_square_root_matrix：\n",
    "#   若你已把 R 的 J 放在 J_R，就用 J_R；否則就用 proj['inverse_square_root_matrix']。\n",
    "P = lik['P']\n",
    "d_hat = lik['d_hat']\n",
    "\n",
    "# 選一個（優先用 R 的 J 做對照）\n",
    "J = J_R if 'J_R' in globals() else proj['inverse_square_root_matrix']\n",
    "\n",
    "# 2) 用 R 的等價公式（列縮放）重算 M / L：\n",
    "M_diag = J @ P @ (torch.diag(d_hat) @ P.T) @ J\n",
    "\n",
    "L_diag_full = Fk @ (torch.diag(torch.sqrt(d_hat)) @ P.T @ J)\n",
    "L_diag = L_diag_full[:, d_hat > 0]   # 與 R 一樣只保留 d_hat > 0 的欄\n",
    "\n",
    "# 3) 列印結果（拿去和 R 的 M / L 對照）\n",
    "print_matrix(\"M_diag_check\", M_diag)\n",
    "print_matrix(\"L_diag_check\", L_diag)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
